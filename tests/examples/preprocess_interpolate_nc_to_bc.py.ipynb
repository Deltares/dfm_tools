{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug 18 16:37:38 2022\n",
    "\n",
    "@author: veenstra\n",
    "\n",
    "This script can be used to interpolate pre-downloaded CMEMS data (or other netcdf files) to points of a pli-file, resulting in a boundary forcingfile (bc file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cc3dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import contextily as ctx\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close('all')\n",
    "import dfm_tools as dfmt\n",
    "import hydrolib.core.dflowfm as hcdfm\n",
    "\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cade725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add coordinate conversion of pli-coordinates? (for nesting RD models in oceanmodels)\n",
    "#TODO: additional models/sources for download/interpolate (evt xESMFÂ for CMCC, climate forcing cmip6 procedure (=calendarconversion) and others)\n",
    "\n",
    "nPoints = 3# None #amount of Points to process per PolyObject in the plifile (use int for testing, use None for all Points)\n",
    "refdate_str = 'minutes since 2011-12-22 00:00:00 +00:00' # if None, xarray uses ds.time.encoding['units'] as refdate_str\n",
    "dir_output = './test_interpolate_nc_to_bc_TEMP'\n",
    "\n",
    "#quantities should be in conversion_dict.keys(). waterlevelbnd is steric/zos, tide is tidal components from FES/EOT\n",
    "# list_quantities = ['waterlevelbnd','salinitybnd','temperaturebnd','uxuy','tracerbndNO3','tide']\n",
    "#list_quantities = ['waterlevelbnd','salinitybnd','temperaturebnd','tracerbndNO3']\n",
    "# list_quantities = ['salinitybnd','tracerbndNO3','tide']\n",
    "#list_quantities = ['waterlevelbnd','salinitybnd','temperaturebnd','uxuy','tracerbndNO3','tracerbndOpal','tracerbndDON','tide'] #also waq vars with same ncvarname, opal not available for GFDL and CMCC\n",
    "list_quantities = ['mesh2d_NO3', 'mesh2d_OXY', 'mesh2d_NH4', 'mesh2d_PO4', 'mesh2d_Si', 'mesh2d_Opal', \n",
    "                   'mesh2d_POC1', 'mesh2d_PON1', 'mesh2d_POP1']  # wq\n",
    "\n",
    "model = 'DCSM' #CMEMS GFDL CMCC HYCOM DCSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c295a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The {ncvarname} wildcard in dir_pattern_hydro/dir_patern_waq is used to replace it with conversion_dict[quantity]['ncvarname'] by using str(dir_pattern).format(ncvarname)\n",
    "list_plifiles = [Path(r'p:\\11208054-004-dcsm-fm\\models\\model_input\\bnd_cond\\pli\\DCSM-FM_OB_all_20181108_nocomments.pli')] #TODO SOLVED: reading this file without '_nocomments' results in empty Polyfile, should raise an error. https://github.com/Deltares/HYDROLIB-core/issues/320\n",
    "if model=='CMEMS': #2012-01-06 12:00:00 to 2013-01-03 12:00:00\n",
    "    conversion_dict = dfmt.get_conversion_dict()\n",
    "    tstart = '2012-01-16 12:00'\n",
    "    tstop = '2012-04-01 12:00'\n",
    "    #tstop = '2013-01-01 12:00'\n",
    "    dir_sourcefiles_hydro = r'p:\\1204257-dcsmzuno\\data\\CMEMS\\nc\\DCSM_allAvailableTimes' #CMEMS hydro: bottomT, so, thetao, uo, vo, zos (2012-01-06 12:00:00 to 2013-01-03 12:00:00) (daily values at noon, not at midnight)\n",
    "    dir_pattern_hydro = Path(dir_sourcefiles_hydro,'{ncvarname}_2012*.nc') # later remove 2012 from string, but this is faster for testing #TODO: it is quite slow, maybe speed up possible?\n",
    "    dir_sourcefiles_waq = r'p:\\11206304-futuremares\\python_scripts\\ocean_boundaryCMEMS\\data_monthly' #CMEMS waq: no3, o2, phyc, so4, si (2011-12-16 12:00:00 to 2019-01-16 12:00:00)\n",
    "    dir_pattern_waq = Path(dir_sourcefiles_waq,'cmems_mod_glo_bgc_my_0.25_P1M-m_{ncvarname}_*.nc') \n",
    "    #to reproduce old CMEMS data (icw reverse_depth=True) (from p:\\1204257-dcsmzuno\\data\\CMEMS\\bnd\\NorthSeaAndBaltic_1993-2019_20210510)\n",
    "    #tstart = dt.datetime(1993,1,1,12,0)\n",
    "    #tstop = tstart + dt.timedelta(days=5)\n",
    "    #dir_pattern_hydro = Path(dir_sourcefiles_hydro,'{ncvarname}_1993*.nc')\n",
    "elif model=='GFDL':\n",
    "    conversion_dict = dfmt.get_conversion_dict()\n",
    "    tstart = '2012-01-16 12:00'\n",
    "    tstop = '2012-04-01 12:00'\n",
    "    dir_sourcefiles_hydro = None\n",
    "    dir_pattern_hydro = None\n",
    "    dir_sourcefiles_waq = r'p:\\11206304-futuremares\\data\\CMIP6_BC\\GFDL-ESM4' #GFDL waq: no3 (1850-01-16 12:00:00 to 2014-12-16 12:00:00)\n",
    "    dir_pattern_waq = Path(dir_sourcefiles_waq,'{ncvarname}_esm-hist.nc')\n",
    "elif model=='CMCC': #TODO: check method, now finding nearest points (so always has values)\n",
    "    #TODO: time_bnds/lev_bnds are available, take into account in bc file?\n",
    "    conversion_dict = dfmt.get_conversion_dict(ncvarname_updates={'salinitybnd':'sos', 'temperaturebnd':'tos'})\n",
    "    conversion_dict['tracerbndNO3'] = {'ncvarname':'no3', 'unit':'g/m3', 'conversion':14.0} #other vars also have different conversion than cmems\n",
    "    tstart = '2015-06-16 12:00'\n",
    "    tstop = '2015-12-01 12:00'\n",
    "    dir_sourcefiles_hydro = r'p:\\11206304-futuremares\\data\\CMIP6_BC\\CMCC-ESM2'\n",
    "    dir_pattern_hydro = Path(dir_sourcefiles_hydro,'{ncvarname}_Omon_CMCC-ESM2_ssp126_r1i1p1f1_gn_*.nc')\n",
    "    dir_sourcefiles_waq = dir_sourcefiles_hydro #CMCC waq: (2015-01-16 12:00:00 to 2100-12-16 12:00:00)\n",
    "    dir_pattern_waq = dir_pattern_hydro\n",
    "elif model=='HYCOM':\n",
    "    list_plifiles = [Path(r'c:\\DATA\\dfm_tools_testdata\\GLBu0.08_expt_91.2\\bcline.pli')] #HYCOM not available in DCSM area, so use other pli-file\n",
    "    conversion_dict = dfmt.get_conversion_dict(ncvarname_updates={'salinitybnd':'salinity', 'temperaturebnd':'water_temp'})\n",
    "    tstart = '2016-04-20'\n",
    "    tstop = '2016-05-03'\n",
    "    dir_sourcefiles_hydro = 'c:\\\\DATA\\\\dfm_tools_testdata\\\\GLBu0.08_expt_91.2' #HYCOM hydro: salinity/so, water_temp/thetao (2016-04-19 00:00:00 to 2016-05-06 00:00:00)\n",
    "    dir_pattern_hydro = Path(dir_sourcefiles_hydro,'HYCOM_ST_GoO_*.nc')\n",
    "    dir_sourcefiles_waq = None\n",
    "    dir_pattern_waq = None\n",
    "elif model=='DCSM':\n",
    "    list_plifiles = [Path(r'p:\\11208479-sequestration-seaweed\\Oosterschelde_DFM_hydro_waq\\dflowfm3d-oosterschelde-wq\\boundary_conditions\\zee-zuid.pli')]\n",
    "    tstart = '2015-04-20'\n",
    "    tstop = '2015-05-03'\n",
    "    dir_sourcefiles_waq = r\"p:\\archivedprojects\\11206044-002ospareutrophication\\final_results_Snellius_latestversion_20220210\"\n",
    "    dir_pattern_waq = [os.path.join(dir_sourcefiles_waq, r'A16b_Numtopsiguniform1_2015_pH\\DFM_OUTPUT_DCSM-FM_0_5nm_waq',f'DCSM-FM_0_5nm_waq_{i:04d}_map.nc') for i in [146,147,148]]\n",
    "else:\n",
    "    raise KeyError(f'invalid model: {model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == 'DCSM':\n",
    "    \n",
    "    # Create a gdf of all the points at every boundary (name, coordinate):\n",
    "    all_boundaries = []\n",
    "    for i,file in list(enumerate(list_plifiles)):  # start with zuid (as did for testing)\n",
    "        print(str(file).split('\\\\')[-1].split('.')[0])\n",
    "        # Read points within this offshore boundary:\n",
    "        pli_file = pd.read_csv(list_plifiles[i], skiprows=1, sep=' ')   # read pli file\n",
    "        pli_file = pli_file.dropna(axis=1)                              # drop empty columns\n",
    "        # Convert dutch crs to decimal degrees (for DCSM)\n",
    "        points = [] \n",
    "        for i in range(0,len(pli_file)):\n",
    "            points.append(Point(pli_file[pli_file.columns[0]].to_list()[i], pli_file[pli_file.columns[1]].to_list()[i]))\n",
    "        d = {'location': pli_file[pli_file.columns[2]].to_list(), 'geometry': points}\n",
    "        gdf = geopandas.GeoDataFrame(d, crs='epsg:28992')   # set Dutch crs\n",
    "        gdf = gdf.to_crs(4326)                              # convert to decimal degrees\n",
    "        all_boundaries.append(gdf)\n",
    "    all_boundaries = pd.concat(all_boundaries, ignore_index=True)\n",
    "    \n",
    "    # Read model and rename\n",
    "    data_xr = dfmt.open_partitioned_dataset(dir_pattern_waq)       # opens every .map file in this folder\n",
    "    data_xr = dfmt.rename_waqvars(waq_xr)\n",
    "    \n",
    "    # Extract boundary points from the model output:\n",
    "    list_point_ds = []\n",
    "    for point in range(0,len(all_boundaries)):        # loop over points\n",
    "        depths_slice = all_xu.mesh2d_layer_z.values\n",
    "    #     if np.isnan(depths_slice).any():\n",
    "    #         raise Exception('nans in depths_slice')\n",
    "        # convert depths (sigma to meter):\n",
    "        ds_atdepths = dfmt.get_Dataset_atdepths(data_xr=all_xu, depths=depths_slice)\n",
    "        # select pli point\n",
    "        ds_atdepths_sel = ds_atdepths.ugrid.sel(x=all_boundaries.geometry[point].x, y=all_boundaries.geometry[point].y)\n",
    "        # rename the variables\n",
    "        ds_atdepths_sel = dfmt.rename_waqvars(ds_atdepths_sel)\n",
    "        ds_atdepths_sel = ds_atdepths_sel.rename({'depth_from_z0':'depth'})\n",
    "        ds_atdepths_sel = ds_atdepths_sel.rename({'mesh2d_nFaces':'plipoints'}) # rename mesh2d_nFaces to plipoints\n",
    "        ds_atdepths_sel['plipoints'] = xr.DataArray([all_boundaries.location[point]], dims='plipoints') # change name of plipoint (node to gdf name)\n",
    "        list_point_ds.append(ds_atdepths_sel)\n",
    "    ds = xr.concat(list_point_ds, dim='plipoints')\n",
    "\n",
    "    # changing the reference time for the output\n",
    "    refdate_str = 'seconds since 1988-01-01 00:00:00'\n",
    "    ds.time.encoding['units'] = refdate_str\n",
    "    \n",
    "    # Loop over the variables and create .bc file:\n",
    "    for num,name in list(enumerate(list_quantities)):    # loop over variables\n",
    "        ForcingModel_object = dfmt.plipointsDataset_to_ForcingModel(plipointsDataset=ds[[list_quantities[num]]])\n",
    "        # Save and write the file\n",
    "        quantity = name.split('_')[-1]\n",
    "        file_bc_basename = all_boundaries.location[point].split('_')[0]\n",
    "        file_bc_out = Path(dir_output,f'{quantity}_{file_bc_basename}_{model}.bc')\n",
    "        ForcingModel_object.save(filepath=file_bc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb10f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of interpolation process\n",
    "dtstart = dt.datetime.now()\n",
    "ext_bnd = hcdfm.ExtModel()\n",
    "os.makedirs(dir_output, exist_ok=True)\n",
    "\n",
    "for file_pli in list_plifiles:\n",
    "    file_bc_basename = file_pli.name.replace('.pli','')\n",
    "    for quantity in list_quantities:\n",
    "        print(f'processing quantity: {quantity}')\n",
    "        if quantity=='tide': \n",
    "            tidemodel = 'FES2014' #FES2014, FES2012, EOT20, GTSM4.1preliminary\n",
    "            if tidemodel == 'FES2014': #for comparing to older FES bc-files #TODO: choose flexible/generic component notation\n",
    "                component_list = ['2n2','mf','p1','m2','mks2','mu2','q1','t2','j1','m3','mm','n2','r2','k1','m4','mn4','s1','k2','m6','ms4','nu2','s2','l2','m8','msf','o1','s4']\n",
    "            else:\n",
    "                component_list = None #None results in all tidemodel components\n",
    "            ForcingModel_object = dfmt.interpolate_tide_to_bc(tidemodel=tidemodel, file_pli=file_pli, component_list=component_list, nPoints=nPoints)\n",
    "            for forcingobject in ForcingModel_object.forcing: #add A0 component\n",
    "                forcingobject.datablock.append(['A0',0.0,0.0])\n",
    "        else:\n",
    "            if quantity in ['waterlevelbnd','salinitybnd','temperaturebnd','uxuy']: #hydro\n",
    "                if dir_sourcefiles_hydro is None:\n",
    "                    continue\n",
    "                if (model=='HYCOM') & (quantity not in ['salinitybnd','temperaturebnd']): #only contains quantities salinity and water_temp, so crashes on others\n",
    "                    continue\n",
    "                dir_pattern = dir_pattern_hydro\n",
    "            else: #waq\n",
    "                if dir_pattern_waq is None:\n",
    "                    continue\n",
    "                dir_pattern = dir_pattern_waq\n",
    "            \n",
    "            #open regulargridDataset and do some basic stuff (time selection, renaming depth/lat/lon/varname, converting units, etc)\n",
    "            data_xr_vars = dfmt.open_dataset_extra(dir_pattern=dir_pattern, quantity=quantity, #TODO: maybe replace renaming part with package CMCC/Lisa?\n",
    "                                                   tstart=tstart, tstop=tstop,\n",
    "                                                   conversion_dict=conversion_dict,\n",
    "                                                   refdate_str=refdate_str)\n",
    "            #interpolate regulargridDataset to plipointsDataset\n",
    "            data_interp = dfmt.interp_regularnc_to_plipoints(data_xr_reg=data_xr_vars, file_pli=file_pli, #TODO: difference in .interp() with float vs da arguments: https://github.com/Deltares/dfm_tools/issues/287\n",
    "                                                             nPoints=nPoints) #argument for testing\n",
    "            #data_interp = data_interp.ffill(dim=\"plipoints\").bfill(dim=\"plipoints\") #to fill allnan plipoints with values from the neighbour point #TODO: this also fills the belowbed layers from one point onto another, so should be done after ffill/bfill in depth dimension. Currently all-nan arrays are replaced with .fillna(0)\n",
    "            \n",
    "            #convert plipointsDataset to hydrolib ForcingModel\n",
    "            ForcingModel_object = dfmt.plipointsDataset_to_ForcingModel(plipointsDataset=data_interp)\n",
    "                    \n",
    "        file_bc_basename = file_pli.name.replace('.pli','')\n",
    "        if quantity=='tide':\n",
    "            file_bc_out = Path(dir_output,f'{quantity}_{file_bc_basename}_{tidemodel}.bc')\n",
    "        else:\n",
    "            file_bc_out = Path(dir_output,f'{quantity}_{file_bc_basename}_{model}.bc')\n",
    "        \n",
    "        print(f'writing ForcingModel to bc file with hydrolib ({file_bc_out.name})')\n",
    "        bc_type = 'bc' #TODO: add netcdf bc support. https://github.com/Deltares/HYDROLIB-core/issues/318\n",
    "        if bc_type=='bc':\n",
    "            #ForcingModel_object.serializer_config.float_format = '.3f' #TODO SOLVED: improve formatting of bc file: https://github.com/Deltares/HYDROLIB-core/issues/308\n",
    "            #ForcingModel_object.serializer_config.float_format_datablock = '.5f' #maybe move this to interp_regularnc_to_plipoints/interpolate_tide_to_bc?\n",
    "            ForcingModel_object.save(filepath=file_bc_out)\n",
    "        \n",
    "        #TODO: support for relative paths?\n",
    "        #generate boundary object for the ext file (quantity, pli-filename, bc-filename)\n",
    "        boundary_object = hcdfm.Boundary(quantity=quantity.replace('tide','waterlevelbnd'), #the FM quantity for tide is also waterlevelbnd\n",
    "                                         locationfile=file_pli,\n",
    "                                         forcingfile=ForcingModel_object)\n",
    "        ext_bnd.boundary.append(boundary_object)\n",
    "\n",
    "        if quantity!='tide': #TODO: data_xr_vars/data_interp does not exist for tide yet\n",
    "            #plotting dataset and polyline (is wrong for CMCC)\n",
    "            varname0 = list(data_xr_vars.data_vars)[0] \n",
    "            fig,ax = plt.subplots()\n",
    "            if 'depth' in data_xr_vars[varname0].dims:\n",
    "                data_xr_vars[varname0].isel(time=0,depth=0).plot(ax=ax)\n",
    "            else:\n",
    "                data_xr_vars[varname0].isel(time=0).plot(ax=ax)\n",
    "            plipoint_coords = data_interp.plipoints.to_dataframe()\n",
    "            ax.plot(plipoint_coords['plipoint_x'],plipoint_coords['plipoint_y'],'r-')\n",
    "            ctx.add_basemap(ax=ax,crs=\"EPSG:4326\",attribution=False)\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(str(file_bc_out).replace('.bc','_polyline'))\n",
    "            \n",
    "            #plotting example data point\n",
    "            for iF in [2]:#range(nPoints): \n",
    "                data_vars = list(data_interp.data_vars)\n",
    "                fig,ax1 = plt.subplots(figsize=(10, 6))\n",
    "                data_interp[data_vars[0]].isel(plipoints=iF).T.plot()\n",
    "                fig.tight_layout()\n",
    "                fig.savefig(str(file_bc_out).replace('.bc',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ext_out = Path(dir_output,'example_bnd.ext')\n",
    "ext_bnd.save(filepath=file_ext_out)\n",
    "\n",
    "time_passed = (dt.datetime.now()-dtstart).total_seconds()\n",
    "print(f'>>total script time passed: {time_passed:.2f} sec')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
