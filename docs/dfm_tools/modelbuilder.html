<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dfm_tools.modelbuilder API documentation</title>
<meta name="description" content="Created on Tue Apr
4 16:12:56 2023 â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dfm_tools.modelbuilder</code></h1>
</header>
<section id="section-intro">
<p>Created on Tue Apr
4 16:12:56 2023</p>
<p>@author: groenenb, sclaan
edited by: veenstra</p>
<p>This is a proof of concept that will be properly coded in the near future and probably end up under hydromt-delft3dfm
Since the functions in this script contain hardcoded parameters, it is not exposed to public and you need to import like dfmt.modelbuilder.[function]</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Created on Tue Apr  4 16:12:56 2023

@author: groenenb, sclaan
edited by: veenstra

This is a proof of concept that will be properly coded in the near future and probably end up under hydromt-delft3dfm
Since the functions in this script contain hardcoded parameters, it is not exposed to public and you need to import like dfmt.modelbuilder.[function]
&#34;&#34;&#34;

import os
import xarray as xr
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import contextily as ctx
import dfm_tools as dfmt
import hydrolib.core.dflowfm as hcdfm
import datetime as dt
import glob
import numpy as np

#gridgen
import meshkernel


#MODELBUILDER: p:\11209231-003-bes-modellering\hydrodynamica\hackathon\preprocessing\scripts\dfm_ModelBuilder_functions.py

def download_meteodata_oceandata(
        longitude_min = 2, longitude_max = 4, latitude_min = 50, latitude_max = 52, # domain
        model = &#39;CMEMS&#39;, #CMEMS ERA5
        overwrite = True, # always set to True when changing the domain
        date_min = &#39;2010-01-01&#39;, #dates as understood by pandas.period_range(). ERA5 has freq=&#39;M&#39; (month) and CMEMS has freq=&#39;D&#39; (day)
        date_max = &#39;2010-01-02&#39;,
        varlist = [], 
        dir_output = &#39;./meteo_ocean_data&#39;,
        make_figs = True
        ):
     
    #download ERA5/CMEMS/HYCOM data for given domain, time extent and variables
    #TODO: add CMCC, GFDL
    #TODO: add climatedata cmip6
    #TODO: add GFS and other NOAA models (https://www.ncei.noaa.gov/products/weather-climate-models/global-forecast &gt; NCEI &gt; TDS)
    #TODO: add click?

    #ERA5
    if model == &#39;ERA5&#39;:
        if isinstance(varlist[0], list):
            varlists = varlist
        else:
            varlists = [varlist]
        
        for varlist in varlists:
            for varkey in varlist:
                if not os.path.isdir(dir_output):
                    os.mkdir(dir_output)
                
                dfmt.download_ERA5(varkey, 
                                   longitude_min=longitude_min-1/4, longitude_max=longitude_max+1/4, latitude_min=latitude_min-1/4, latitude_max=latitude_max+1/4, # download 1 grid cell row/column extra
                                   date_min=date_min, date_max=date_max,
                                   dir_output=dir_output, overwrite=overwrite)
            
                #open mfdataset to check folder contents
                if make_figs:
                    ds = xr.open_mfdataset(os.path.join(dir_output,f&#39;era5_{varkey}_*.nc&#39;))
                    ds.close()
                    fig,ax = plt.subplots()
                    ds[varkey].isel(time=0).plot(ax=ax)
                    ctx.add_basemap(ax=ax,crs=&#34;EPSG:4326&#34;,attribution=False)
                    #TODO    file_out = os.path.join(dir_output, f&#39;era5_{varkey}_{time_start_str}to{time_stop_str}&#39;)
                    #TODO    name_output = f&#39;era5_{varkey}_{dt.datetime.strftime(ds[varkey].time[0],&#34;%Y-%m&#34;)}.nc&#39;
                    #TODO    fig.savefig(file_out)
    
    
    #CMEMS
    if model == &#39;CMEMS&#39;:
        date_min_cmems = pd.Timestamp(date_min)-pd.Timedelta(days=1) #CMEMS has daily noon values (not midnight), so subtract one day from date_min to cover desired time extent
        for varkey in varlist:
            Path(dir_output).mkdir(parents=True, exist_ok=True)
            #TODO: update CMEMS urls after 15 april, since some are being replaced
            if varkey in [&#39;bottomT&#39;,&#39;mlotst&#39;,&#39;siconc&#39;,&#39;sithick&#39;,&#39;so&#39;,&#39;thetao&#39;,&#39;uo&#39;,&#39;usi&#39;,&#39;vo&#39;,&#39;vsi&#39;,&#39;zos&#39;]: #for physchem
                #reanalisys: https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description
                #dataset_url = &#39;https://my.cmems-du.eu/thredds/dodsC/cmems_mod_glo_phy_my_0.083_P1D-m&#39;
                #forecast: https://data.marine.copernicus.eu/product/GLOBAL_ANALYSISFORECAST_PHY_001_024/description
                dataset_url = &#39;https://nrt.cmems-du.eu/thredds/dodsC/global-analysis-forecast-phy-001-024&#39; #old location for forecasts (available up to 15 april)
                #dataset_url = &#39;https://nrt.cmems-du.eu/thredds/dodsC/cmems_mod_glo_phy_anfc_0.083deg_PT1H-m.html&#39; #hourly for cur/tem/sal (surface only)
                #dataset_url = &#39;https://nrt.cmems-du.eu/thredds/dodsC/cmems_mod_glo_phy-cur_anfc_0.083deg_P1D-m.html&#39; #currents
                #dataset_url = &#39;https://nrt.cmems-du.eu/thredds/dodsC/cmems_mod_glo_phy-thetao_anfc_0.083deg_P1D-m.html&#39; #temperature
                #dataset_url = &#39;https://nrt.cmems-du.eu/thredds/dodsC/cmems_mod_glo_phy-so_anfc_0.083deg_P1D-m.html&#39; #salinity
            else: # for bio #https://data.marine.copernicus.eu/product/GLOBAL_ANALYSIS_FORECAST_BIO_001_028/description
                dataset_url = &#39;https://my.cmems-du.eu/thredds/dodsC/cmems_mod_glo_bgc_my_0.25_P1D-m&#39; #contains [&#39;chl&#39;,&#39;no3&#39;,&#39;nppv&#39;,&#39;o2&#39;,&#39;po4&#39;,&#39;si&#39;]
                #dataset_url = &#39;https://nrt.cmems-du.eu/thredds/dodsC/global-analysis-forecast-bio-001-028-daily&#39; #contains [&#39;chl&#39;,&#39;fe&#39;,&#39;no3&#39;,&#39;nppv&#39;,&#39;o2&#39;,&#39;ph&#39;,&#39;phyc&#39;,&#39;po4&#39;,&#39;si&#39;,&#39;spco2&#39;]
            file_prefix = &#39;cmems_&#39;
            
            dfmt.download_OPeNDAP(dataset_url=dataset_url,
                                  credentials=None, #credentials=[&#39;username&#39;,&#39;password&#39;], or create &#34;%USERPROFILE%/CMEMS_credentials.txt&#34; with username on line 1 and password on line 2. Register at: https://resources.marine.copernicus.eu/registration-form&#39;
                                  varkey=varkey,
                                  longitude_min=longitude_min-1/12, longitude_max=longitude_max+1/12, latitude_min=latitude_min-1/12, latitude_max=latitude_max+1/12, # download 1 grid cell row/column extra
                                  date_min=date_min_cmems, date_max=date_max,
                                  dir_output=dir_output, file_prefix=file_prefix, overwrite=overwrite)
            
            #open mfdataset to check folder contents and plot first field of each variable
            if make_figs:
                ds = xr.open_mfdataset(os.path.join(dir_output,f&#39;{file_prefix}{varkey}_*.nc&#39;))
                fig,ax = plt.subplots()
                if &#39;depth&#39; in ds[varkey].dims:
                    ds[varkey].isel(time=0,depth=0).plot(ax=ax)
                else:
                    ds[varkey].isel(time=0).plot(ax=ax)
                ds.close()
                ctx.add_basemap(ax=ax,crs=&#34;EPSG:4326&#34;,attribution=False)
        
    
    #HYCOM
    if model == &#39;HYCOM&#39;:
        for varkey in varlist:
            Path(dir_output).mkdir(parents=True, exist_ok=True)
            
            period_range_years = pd.period_range(date_min,date_max,freq=&#39;Y&#39;)
            dataset_url = [f&#39;https://tds.hycom.org/thredds/dodsC/GLBu0.08/expt_19.1/{year}&#39; for year in period_range_years] #list is possible with hycom, since it uses xr.open_mfdataset()
            file_prefix = &#39;hycom_&#39;
            
            dfmt.download_OPeNDAP(dataset_url=dataset_url,
                                  varkey=varkey,
                                  longitude_min=longitude_min, longitude_max=longitude_max, latitude_min=latitude_min, latitude_max=latitude_max,
                                  date_min=date_min, date_max=date_max,
                                  dir_output=dir_output, file_prefix=file_prefix, overwrite=overwrite)
            
            #open mfdataset to check folder contents and plot first field of each variable
            if make_figs:
                ds = xr.open_mfdataset(os.path.join(dir_output,f&#39;{file_prefix}{varkey}_*.nc&#39;))
                fig,ax = plt.subplots()
                if &#39;depth&#39; in ds[varkey].dims:
                    ds[varkey].isel(time=0,depth=0).plot(ax=ax)
                else:
                    ds[varkey].isel(time=0).plot(ax=ax)
                ds.close()
                ctx.add_basemap(ax=ax,crs=&#34;EPSG:4326&#34;,attribution=False)
    

    
    
def preprocess_interpolate_nc_to_bc(
        ext_bnd,
        refdate_str = &#39;minutes since 2000-01-01 00:00:00 +00:00&#39;, # if None, xarray uses ds.time.encoding[&#39;units&#39;] as refdate_str
        dir_output = &#39;./interpolate_nc_to_bc&#39;,
        #quantities should be in conversion_dict.keys(). waterlevelbnd is steric/zos, tide is tidal components from FES/EOT
        list_quantities = [&#39;waterlevelbnd&#39;,&#39;salinitybnd&#39;,&#39;temperaturebnd&#39;,&#39;uxuy&#39;,&#39;tide&#39;], # e.g. [&#39;waterlevelbnd&#39;,&#39;salinitybnd&#39;,&#39;temperaturebnd&#39;,&#39;uxuy&#39;,&#39;tide&#39;,&#39;tracerbndNO3&#39;,&#39;tracerbndOpal&#39;,&#39;tracerbndDON&#39;]
        model = &#39;CMEMS&#39;, #CMEMS GFDL CMCC HYCOM
        tstart = &#39;2012-01-01 12:00&#39;,
        tstop = &#39;2012-01-02 12:00&#39;,
        list_plifiles = [r&#39;p:\11208054-004-dcsm-fm\models\model_input\bnd_cond\pli\DCSM-FM_OB_all_20181108_nocomments.pli&#39;], #TODO: reading this file without &#39;_nocomments&#39; results in empty Polyfile, should raise an error. https://github.com/Deltares/HYDROLIB-core/issues/320
        dir_sourcefiles_hydro = r&#39;p:\1204257-dcsmzuno\data\CMEMS\nc\DCSM_allAvailableTimes&#39;, #CMEMS hydro: bottomT, so, thetao, uo, vo, zos (2012-01-06 12:00:00 to 2013-01-03 12:00:00) (daily values at noon, not at midnight)
        make_figs = True): 
    
    list_plifiles = [Path(x) for x in list_plifiles]
    
    #TODO: add coordinate conversion of pli-coordinates? (for nesting RD models in oceanmodels)
    #TODO: additional models/sources for download/interpolate (evt xESMF for CMCC, climate forcing cmip6 procedure (=calendarconversion) and others)
    
    #The {ncvarname} wildcard in dir_pattern_hydro/dir_patern_waq is used to replace it with conversion_dict[quantity][&#39;ncvarname&#39;] by using str(dir_pattern).format(ncvarname)
    if model==&#39;CMEMS&#39;: #2012-01-06 12:00:00 to 2013-01-03 12:00:00
        conversion_dict = dfmt.get_conversion_dict()
        dir_pattern_hydro = Path(dir_sourcefiles_hydro,&#39;cmems_{ncvarname}_*.nc&#39;) # later remove 2012 from string, but this is faster for testing #TODO: it is quite slow, maybe speed up possible?
        dir_sourcefiles_waq = r&#39;p:\11206304-futuremares\python_scripts\ocean_boundaryCMEMS\data_monthly&#39; #CMEMS waq: no3, o2, phyc, so4, si (2011-12-16 12:00:00 to 2019-01-16 12:00:00)
        dir_pattern_waq = Path(dir_sourcefiles_waq,&#39;cmems_mod_glo_bgc_my_0.25_P1M-m_{ncvarname}_*.nc&#39;) 
        #to reproduce old CMEMS data (icw reverse_depth=True) (from p:\1204257-dcsmzuno\data\CMEMS\bnd\NorthSeaAndBaltic_1993-2019_20210510)
        #tstart = dt.datetime(1993,1,1,12,0)
        #tstop = tstart + dt.timedelta(days=5)
        #dir_pattern_hydro = Path(dir_sourcefiles_hydro,&#39;{ncvarname}_1993*.nc&#39;)
        #TODO tstart = pd.Timestamp(tstart)-pd.Timedelta(days=1) #similar as download: CMEMS has daily noon values (not midnight), so subtract one day from date_min to cover desired time extent
    elif model==&#39;GFDL&#39;:
        conversion_dict = dfmt.get_conversion_dict()
        tstart = &#39;2012-01-16 12:00&#39;
        tstop = &#39;2012-04-01 12:00&#39;
        list_plifiles = [Path(r&#39;p:\11208054-004-dcsm-fm\models\model_input\bnd_cond\pli\DCSM-FM_OB_all_20181108_nocomments.pli&#39;)] #TODO: reading this file without &#39;_nocomments&#39; results in empty Polyfile, should raise an error. https://github.com/Deltares/HYDROLIB-core/issues/320
        dir_sourcefiles_hydro = None
        dir_pattern_hydro = None
        dir_sourcefiles_waq = r&#39;p:\11206304-futuremares\data\CMIP6_BC\GFDL-ESM4&#39; #GFDL waq: no3 (1850-01-16 12:00:00 to 2014-12-16 12:00:00)
        dir_pattern_waq = Path(dir_sourcefiles_waq,&#39;{ncvarname}_esm-hist.nc&#39;)
    elif model==&#39;CMCC&#39;: #TODO: check method, now finding nearest points (so always has values)
        #TODO: time_bnds/lev_bnds are available, take into account in bc file?
        conversion_dict = dfmt.get_conversion_dict(ncvarname_updates={&#39;salinitybnd&#39;:&#39;sos&#39;, &#39;temperaturebnd&#39;:&#39;tos&#39;})
        conversion_dict[&#39;tracerbndNO3&#39;] = {&#39;ncvarname&#39;:&#39;no3&#39;, &#39;unit&#39;:&#39;g/m3&#39;, &#39;conversion&#39;:14.0} #other vars also have different conversion than cmems
        tstart = &#39;2015-06-16 12:00&#39;
        tstop = &#39;2015-12-01 12:00&#39;
        list_plifiles = [Path(r&#39;p:\11208054-004-dcsm-fm\models\model_input\bnd_cond\pli\DCSM-FM_OB_all_20181108_nocomments.pli&#39;)] #TODO: reading this file without &#39;_nocomments&#39; results in empty Polyfile, should raise an error. https://github.com/Deltares/HYDROLIB-core/issues/320
        dir_sourcefiles_hydro = r&#39;p:\11206304-futuremares\data\CMIP6_BC\CMCC-ESM2&#39;
        dir_pattern_hydro = Path(dir_sourcefiles_hydro,&#39;{ncvarname}_Omon_CMCC-ESM2_ssp126_r1i1p1f1_gn_*.nc&#39;)
        dir_sourcefiles_waq = dir_sourcefiles_hydro #CMCC waq: (2015-01-16 12:00:00 to 2100-12-16 12:00:00)
        dir_pattern_waq = dir_pattern_hydro
    elif model==&#39;HYCOM&#39;:
        conversion_dict = dfmt.get_conversion_dict(ncvarname_updates={&#39;salinitybnd&#39;:&#39;salinity&#39;, &#39;temperaturebnd&#39;:&#39;water_temp&#39;})
        tstart = &#39;2016-04-20&#39;
        tstop = &#39;2016-05-03&#39;
        list_plifiles = [Path(r&#39;c:\DATA\dfm_tools_testdata\GLBu0.08_expt_91.2\bcline.pli&#39;)] #HYCOM not available in DCSM area, so use other pli-file
        dir_sourcefiles_hydro = &#39;c:\\DATA\\dfm_tools_testdata\\GLBu0.08_expt_91.2&#39; #HYCOM hydro: salinity/so, water_temp/thetao (2016-04-19 00:00:00 to 2016-05-06 00:00:00)
        dir_pattern_hydro = Path(dir_sourcefiles_hydro,&#39;HYCOM_ST_GoO_*.nc&#39;)
        dir_sourcefiles_waq = None
        dir_pattern_waq = None
    else:
        raise Exception(f&#39;invalid model: {model}&#39;)
    
    # start of interpolation process
    dtstart = dt.datetime.now()
    if not os.path.isdir(dir_output):
        os.mkdir(dir_output)
    
    for file_pli in list_plifiles:
        file_bc_basename = file_pli.name.replace(&#39;.pli&#39;,&#39;&#39;)
        for quantity in list_quantities:
            print(f&#39;processing quantity: {quantity}&#39;)
            if quantity==&#39;tide&#39;: 
                tidemodel = &#39;FES2014&#39; #FES2014, FES2012, EOT20, GTSM4.1preliminary
                if tidemodel == &#39;FES2014&#39;: #for comparing to older FES bc-files #TODO: choose flexible/generic component notation
                    component_list = [&#39;2n2&#39;,&#39;mf&#39;,&#39;p1&#39;,&#39;m2&#39;,&#39;mks2&#39;,&#39;mu2&#39;,&#39;q1&#39;,&#39;t2&#39;,&#39;j1&#39;,&#39;m3&#39;,&#39;mm&#39;,&#39;n2&#39;,&#39;r2&#39;,&#39;k1&#39;,&#39;m4&#39;,&#39;mn4&#39;,&#39;s1&#39;,&#39;k2&#39;,&#39;m6&#39;,&#39;ms4&#39;,&#39;nu2&#39;,&#39;s2&#39;,&#39;l2&#39;,&#39;m8&#39;,&#39;msf&#39;,&#39;o1&#39;,&#39;s4&#39;]
                else:
                    component_list = None #None results in all tidemodel components
                ForcingModel_object = dfmt.interpolate_tide_to_bc(tidemodel=tidemodel, file_pli=file_pli, component_list=component_list, nPoints=None)
                for forcingobject in ForcingModel_object.forcing: #add A0 component
                    forcingobject.datablock.append([&#39;A0&#39;,0.0,0.0])
            else:
                if quantity in [&#39;waterlevelbnd&#39;,&#39;salinitybnd&#39;,&#39;temperaturebnd&#39;,&#39;uxuy&#39;]: #hydro
                    if dir_sourcefiles_hydro is None:
                        continue
                    if (model==&#39;HYCOM&#39;) &amp; (quantity not in [&#39;salinitybnd&#39;,&#39;temperaturebnd&#39;]): #only contains quantities salinity and water_temp, so crashes on others
                        continue
                    dir_pattern = dir_pattern_hydro
                else: #waq
                    if dir_pattern_waq is None:
                        continue
                    dir_pattern = dir_pattern_waq
                
                #open regulargridDataset and do some basic stuff (time selection, renaming depth/lat/lon/varname, converting units, etc)
                data_xr_vars = dfmt.open_dataset_extra(dir_pattern=dir_pattern, quantity=quantity, #TODO: maybe replace renaming part with package CMCC/Lisa?
                                                       tstart=tstart, tstop=tstop,
                                                       conversion_dict=conversion_dict,
                                                       refdate_str=refdate_str)
                #interpolate regulargridDataset to plipointsDataset
                data_interp = dfmt.interp_regularnc_to_plipoints(data_xr_reg=data_xr_vars, file_pli=file_pli, #TODO: difference in .interp() with float vs da arguments: https://github.com/Deltares/dfm_tools/issues/287
                                                                 nPoints=None) #argument for testing
                
                #convert plipointsDataset to hydrolib ForcingModel
                ForcingModel_object = dfmt.plipointsDataset_to_ForcingModel(plipointsDataset=data_interp)
                        
            file_bc_basename = file_pli.name.replace(&#39;.pli&#39;,&#39;&#39;)
            if quantity==&#39;tide&#39;:
                file_bc_out = Path(dir_output,f&#39;{quantity}_{file_bc_basename}_{tidemodel}.bc&#39;)
            else:
                file_bc_out = Path(dir_output,f&#39;{quantity}_{file_bc_basename}_{model}.bc&#39;)
            
            print(f&#39;writing ForcingModel to bc file with hydrolib ({file_bc_out.name})&#39;)
            bc_type = &#39;bc&#39; #TODO: add netcdf bc support. https://github.com/Deltares/HYDROLIB-core/issues/318
            if bc_type==&#39;bc&#39;:
                #ForcingModel_object.serializer_config.float_format = &#39;.3f&#39; #TODO SOLVED: improve formatting of bc file: https://github.com/Deltares/HYDROLIB-core/issues/308
                #ForcingModel_object.serializer_config.float_format_datablock = &#39;.5f&#39; #maybe move this to interp_regularnc_to_plipoints/interpolate_tide_to_bc?
                ForcingModel_object.save(filepath=file_bc_out)
            
            #TODO: support for relative paths?
            #generate boundary object for the ext file (quantity, pli-filename, bc-filename)
            boundary_object = hcdfm.Boundary(quantity=quantity.replace(&#39;tide&#39;,&#39;waterlevelbnd&#39;), #the FM quantity for tide is also waterlevelbnd
                                             locationfile=file_pli,
                                             forcingfile=ForcingModel_object)
            ext_bnd.boundary.append(boundary_object)
    
            if make_figs and quantity!=&#39;tide&#39;: #TODO: data_xr_vars/data_interp does not exist for tide yet
                #plotting dataset and polyline (is wrong for CMCC)
                varname0 = list(data_xr_vars.data_vars)[0] 
                fig,ax = plt.subplots()
                if &#39;depth&#39; in data_xr_vars[varname0].dims:
                    data_xr_vars[varname0].isel(time=0,depth=0).plot(ax=ax)
                else:
                    data_xr_vars[varname0].isel(time=0).plot(ax=ax)
                plipoint_coords = data_interp.plipoints.to_dataframe()
                ax.plot(plipoint_coords[&#39;plipoint_x&#39;],plipoint_coords[&#39;plipoint_y&#39;],&#39;r-&#39;)
                ctx.add_basemap(ax=ax,crs=&#34;EPSG:4326&#34;,attribution=False)
                fig.tight_layout()
                fig.savefig(str(file_bc_out).replace(&#39;.bc&#39;,&#39;_polyline&#39;))
                
                #plotting example data point
                for iF in [2]:#range(nPoints): 
                    data_vars = list(data_interp.data_vars)
                    fig,ax1 = plt.subplots(figsize=(10, 6))
                    data_interp[data_vars[0]].isel(plipoints=iF).T.plot()
                    fig.tight_layout()
                    fig.savefig(str(file_bc_out).replace(&#39;.bc&#39;,&#39;&#39;))
    
    #file_ext_out = Path(dir_output,&#39;example_bnd.ext&#39;)
    #ext_bnd.save(filepath=file_ext_out)
    
    time_passed = (dt.datetime.now()-dtstart).total_seconds()
    print(f&#39;&gt;&gt;total script time passed: {time_passed:.2f} sec&#39;)
    return ext_bnd

    
def preprocess_ini_cmems_to_nc(tSimStart = dt.datetime(1998,1,1),
        dir_data  = r&#39;p:\i1000668-tcoms\03_newModel\01_input\02_bnd\data_opendap&#39;, #folder containing CMEMS so and thetao netcdf files
        dir_out = &#39;.&#39;):
    #TODO: merge with other ini script and make generic for getting an inifield out of CMEMS/etc regulargrid Dataset or a 2D/3D FM map/rst Dataset
    
    file_nc_list_so = glob.glob(f&#39;{dir_data}\\cmems_so_*.nc&#39;)
    file_nc_list_thetao = glob.glob(f&#39;{dir_data}\\cmems_thetao_*.nc&#39;)
    file_nc_list = file_nc_list_so + file_nc_list_thetao
    
    print(f&#39;opening {len(file_nc_list)} datasets&#39;)
    data_xr = xr.open_mfdataset(file_nc_list)
    
    if 0: #this would be the proper way to do it, but FM needs two timesteps for some reason
        print(&#39;ds.interp()&#39;)
        data_xr_ontime = data_xr.interp(time=[tSimStart],kwargs=dict(bounds_error=True)) #bounds_error makes sure, outofbounds time results in &#34;ValueError: A value in x_new is below the interpolation range.&#34;
    else:
        print(&#39;ds.sel()&#39;)
        data_xr_ontime = data_xr.sel(time=slice(tSimStart-dt.timedelta(days=1),tSimStart+dt.timedelta(days=1)))
    
    print(&#39;writing file&#39;)
    outFile = os.path.join(dir_out,f&#39;InitialField_{tSimStart.strftime(&#34;%Y-%m-%d_%H-%M-%S&#34;)}.nc&#39;)
    data_xr_ontime.to_netcdf(outFile,format=&#34;NETCDF4_CLASSIC&#34;)
    
    
def preprocess_merge_meteofiles(
        mode = &#39;HYCOM&#39;, # &#39;HIRLAM_meteo&#39; &#39;HIRLAM_meteo-heatflux&#39; &#39;HARMONIE&#39; &#39;HYCOM&#39; &#39;ERA5_wind_pressure&#39; &#39;ERA5_heat_model&#39; &#39;ERA5_radiation&#39; &#39;ERA5_rainfall&#39; &#39;WOA&#39;
        varkey_list = [],
        dir_data = [],
        dir_output = &#39;.&#39;,
        time_slice = slice(&#39;2013-12-30&#39;,&#39;2014-01-01&#39;)
        ):

    if isinstance(varkey_list[0], list):
        varkey_lists = varkey_list
    else:
        varkey_list = [varkey_list]
        
    for varkey_list in varkey_lists:
        if &#39;HIRLAM&#39; in mode:
            if mode == &#39;HIRLAM_meteo&#39;: #1year voor meteo crasht (HIRLAM72_*\\h72_*) door conflicting dimension sizes, sourcefolders opruimen? meteo_heatflux folders zijn schoner dus daar werkt het wel
                dir_data = &#39;P:\\1204257-dcsmzuno\\2014\\data\\meteo\\HIRLAM72_*&#39; #files contain: [&#39;air_pressure_fixed_height&#39;,&#39;northward_wind&#39;,&#39;eastward_wind&#39;]
            elif mode == &#39;HIRLAM_meteo-heatflux&#39;:
                dir_data = &#39;P:\\1204257-dcsmzuno\\2014\\data\\meteo-heatflux\\HIRLAM72_*&#39; # files contain: [&#39;dew_point_temperature&#39;,&#39;air_temperature&#39;,&#39;cloud_area_fraction&#39;]
            fn_match_pattern = &#39;h72_20131*.nc&#39;
            file_out_prefix = &#39;h72_&#39;
            preprocess = dfmt.preprocess_hirlam #temporary(?) fix for &gt;1D-vars with same name as its dim
        elif mode == &#39;HARMONIE&#39;:
            dir_data = &#39;p:\\1204257-dcsmzuno\\data\\meteo\\HARMONIE\\nc\\air_*&#39; #many invalid files, so subsetting here
            fn_match_pattern = &#39;HARMONIE_*_2020_*.nc&#39;
            file_out_prefix = &#39;HARMONIE_&#39;
            preprocess = None
        elif mode == &#39;HYCOM&#39;:
            dir_data = &#39;c:\\DATA\\dfm_tools_testdata\\GLBu0.08_expt_91.2&#39;
            fn_match_pattern = &#39;HYCOM_ST_GoO_*.nc&#39;
            file_out_prefix = &#39;HYCOM_ST_GoO_&#39;
            preprocess = None
            #rename_variables = {&#39;salinity&#39;:&#39;so&#39;, &#39;water_temp&#39;:&#39;thetao&#39;}
        elif &#39;ERA5&#39; in mode:
            fn_match_pattern = f&#39;era5_.*({&#34;|&#34;.join(varkey_list)})_.*.nc&#39; #simpler but selects more files: &#39;era5_*.nc&#39;
            file_out_prefix = f&#39;era5_{&#34;_&#34;.join(varkey_list)}_&#39;
            preprocess = dfmt.preprocess_ERA5 #reduce expver dimension if present
        elif mode == &#39;WOA&#39;:
            dir_data = r&#39;p:\1204257-dcsmzuno\data\WOA13&#39;
            fn_match_pattern = &#39;woa13_decav_s*.nc&#39;
            file_out_prefix = &#39;woa13_decav_s_&#39;
            preprocess = dfmt.preprocess_woa #add 360-day calendar unit to time attrs before decode_cf
        else:
            raise Exception(&#39;ERROR: wrong mode %s&#39;%(mode))
        
        if not os.path.exists(dir_output):
            os.makedirs(dir_output)
        
        file_nc = os.path.join(dir_data,fn_match_pattern)
        
        data_xr_tsel = dfmt.merge_meteofiles(file_nc=file_nc, time_slice=time_slice, 
                                             preprocess=preprocess,
                                             add_global_overlap=False, #GTSM specific: extend data beyond -180 to 180 longitude
                                             zerostart=False) #GTSM specific: extend data with 0-value fields 1 and 2 days before all_tstart
        
        #write to netcdf file
        print(&#39;&gt;&gt; writing file (can take a while): &#39;,end=&#39;&#39;)
        dtstart = dt.datetime.now()
        try:
            times_np = data_xr_tsel[&#39;time&#39;].to_series()
        except:
            times_np = data_xr_tsel[&#39;time&#39;].to_numpy() #.to_series() does not work for woa 360_day data. .to_numpy() results in numpy.datetime64 for other datasets, which has no attribute strftime
        time_start_str = times_np[0].strftime(&#34;%Y%m%d&#34;)
        time_stop_str = times_np[-1].strftime(&#34;%Y%m%d&#34;)
        file_out = os.path.join(dir_output, f&#39;{file_out_prefix}{time_start_str}to{time_stop_str}_{mode}.nc&#39;)
        data_xr_tsel.to_netcdf(file_out)
        print(f&#39;{(dt.datetime.now()-dtstart).total_seconds():.2f} sec&#39;)
        
        
        #load outputfile
        data_xr_check = xr.open_dataset(file_out)
        
        for varkey in data_xr_check.data_vars:
            varsel = data_xr_check[varkey]
            if not set([&#39;longitude&#39;,&#39;latitude&#39;]).issubset(set(varsel.coords)): #skipping vars without lat/lon coordinate
                continue
            print(f&#39;plotting {varkey}&#39;)
            fig,ax1 = plt.subplots()
            if &#39;HIRLAM&#39; in mode:
                varsel.isel(time=0).plot(ax=ax1,x=&#39;longitude&#39;,y=&#39;latitude&#39;) #x/y are necessary since coords are not 1D and dims
            elif &#39;depth&#39; in data_xr_tsel[varkey].coords:
                varsel.isel(time=0).sel(depth=0).plot(ax=ax1)
            else:
                varsel.isel(time=0).plot(ax=ax1)
            file_out = os.path.join(dir_output, f&#39;era5_{varkey}_{time_start_str}&#39;)
            fig.savefig(file_out)



#GRIDGENERATION WITH MESHKERNEL

def make_basegrid(lon_min,lon_max,lat_min,lat_max,dx=0.05,dy=0.05,angle=0):
    print(&#39;modelbuilder.make_basegrid()&#39;)
    # create base grid
    nox = int(np.round((lon_max-lon_min)/dx))
    noy = int(np.round((lat_max-lat_min)/dy))
    
    make_grid_parameters = meshkernel.MakeGridParameters(num_columns=nox,
                                                         num_rows=noy,
                                                         angle=angle,
                                                         origin_x=lon_min,
                                                         origin_y=lat_min,
                                                         block_size_x=dx,
                                                         block_size_y=dy)
    #print(make_grid_parameters)
    
    geometry_list = meshkernel.GeometryList(np.empty(0, dtype=np.double), np.empty(0, dtype=np.double)) # A polygon must to be provided. If empty it will not be used. If a polygon is provided it will be used in the generation of the curvilinear grid. The polygon must be closed
    mk = meshkernel.MeshKernel() #TODO: is_geographic=True was used in modelbuilder, but refinement super slow and raises &#34;MeshKernelError: MeshRefinement::connect_hanging_nodes: The number of non-hanging nodes is neither 3 nor 4.&#34;
    mk.curvilinear_make_uniform(make_grid_parameters, geometry_list) #TODO: make geometry_list argument optional: https://github.com/Deltares/MeshKernelPy/issues/30
    mk.curvilinear_convert_to_mesh2d() #convert to ugrid/mesh2d
    
    #plot
    mesh2d_mesh_kernel = mk.mesh2d_get()
    fig, ax = plt.subplots()
    mesh2d_mesh_kernel.plot_edges(ax, color=&#39;blue&#39;)
    source = ctx.providers.Esri.WorldImagery
    ctx.add_basemap(ax=ax, source=source, crs=&#39;EPSG:4326&#39;, attribution=False)
    
    return mk


def refine_basegrid(mk, data_bathy_sel,min_face_size=0.1):
    print(&#39;modelbuilder.refine_basegrid()&#39;)
    samp_x,samp_y = np.meshgrid(data_bathy_sel.lon.to_numpy(),data_bathy_sel.lat.to_numpy())
    samp_z = data_bathy_sel.elevation.to_numpy().astype(float) #TODO: without .astype(float), meshkernelpy generates &#34;TypeError: incompatible types, c_short_Array_27120 instance instead of LP_c_double instance&#34;: https://github.com/Deltares/MeshKernelPy/issues/31
    samp_x = samp_x.ravel()
    samp_y = samp_y.ravel()
    samp_z = samp_z.ravel()
    geomlist = meshkernel.GeometryList(x_coordinates=samp_x, y_coordinates=samp_y, values=samp_z) #TODO: does not check if lenghts of input array is equal (samp_z[1:]) https://github.com/Deltares/MeshKernelPy/issues/32
    
    #refinement
    mesh_refinement_parameters = meshkernel.MeshRefinementParameters(refine_intersected=False, #TODO: provide defaults for several arguments, so less arguments are required
                                                                     use_mass_center_when_refining=False, #TODO: what does this do?
                                                                     min_face_size=min_face_size, #TODO: size in meters would be more convenient: https://github.com/Deltares/MeshKernelPy/issues/33
                                                                     refinement_type=meshkernel.RefinementType(1), #Wavecourant/1,
                                                                     connect_hanging_nodes=True, #set to False to do multiple refinement steps (e.g. for multiple regions)
                                                                     account_for_samples_outside_face=True, #outsidecell argument for --refine?
                                                                     max_refinement_iterations=5,
                                                                     ) #TODO: missing the arguments dtmax (necessary?), hmin (min_face_size but then in meters instead of degrees), smoothiters (currently refinement is patchy along coastlines, goes good in dflowfm exec after additional implementation of HK), spherical 1/0 (necessary?)
    
    mk.mesh2d_refine_based_on_samples(samples=geomlist,
                                       relative_search_radius=0.5, #TODO: bilin interp is preferred, but this is currently not supported (samples have to be ravelled): https://github.com/Deltares/MeshKernelPy/issues/34
                                       minimum_num_samples=3,
                                       mesh_refinement_params=mesh_refinement_parameters,
                                       )
    
    #plotting
    mesh2d_grid2 = mk.mesh2d_get()
    fig, ax = plt.subplots()
    mesh2d_grid2.plot_edges(ax,linewidth=1.2)
    ctx.add_basemap(ax=ax, crs=&#39;EPSG:4326&#39;, attribution=False)
    
    return mk


    </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dfm_tools.modelbuilder.download_meteodata_oceandata"><code class="name flex">
<span>def <span class="ident">download_meteodata_oceandata</span></span>(<span>longitude_min=2, longitude_max=4, latitude_min=50, latitude_max=52, model='CMEMS', overwrite=True, date_min='2010-01-01', date_max='2010-01-02', varlist=[], dir_output='./meteo_ocean_data', make_figs=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def download_meteodata_oceandata(
        longitude_min = 2, longitude_max = 4, latitude_min = 50, latitude_max = 52, # domain
        model = &#39;CMEMS&#39;, #CMEMS ERA5
        overwrite = True, # always set to True when changing the domain
        date_min = &#39;2010-01-01&#39;, #dates as understood by pandas.period_range(). ERA5 has freq=&#39;M&#39; (month) and CMEMS has freq=&#39;D&#39; (day)
        date_max = &#39;2010-01-02&#39;,
        varlist = [], 
        dir_output = &#39;./meteo_ocean_data&#39;,
        make_figs = True
        ):
     
    #download ERA5/CMEMS/HYCOM data for given domain, time extent and variables
    #TODO: add CMCC, GFDL
    #TODO: add climatedata cmip6
    #TODO: add GFS and other NOAA models (https://www.ncei.noaa.gov/products/weather-climate-models/global-forecast &gt; NCEI &gt; TDS)
    #TODO: add click?

    #ERA5
    if model == &#39;ERA5&#39;:
        if isinstance(varlist[0], list):
            varlists = varlist
        else:
            varlists = [varlist]
        
        for varlist in varlists:
            for varkey in varlist:
                if not os.path.isdir(dir_output):
                    os.mkdir(dir_output)
                
                dfmt.download_ERA5(varkey, 
                                   longitude_min=longitude_min-1/4, longitude_max=longitude_max+1/4, latitude_min=latitude_min-1/4, latitude_max=latitude_max+1/4, # download 1 grid cell row/column extra
                                   date_min=date_min, date_max=date_max,
                                   dir_output=dir_output, overwrite=overwrite)
            
                #open mfdataset to check folder contents
                if make_figs:
                    ds = xr.open_mfdataset(os.path.join(dir_output,f&#39;era5_{varkey}_*.nc&#39;))
                    ds.close()
                    fig,ax = plt.subplots()
                    ds[varkey].isel(time=0).plot(ax=ax)
                    ctx.add_basemap(ax=ax,crs=&#34;EPSG:4326&#34;,attribution=False)
                    #TODO    file_out = os.path.join(dir_output, f&#39;era5_{varkey}_{time_start_str}to{time_stop_str}&#39;)
                    #TODO    name_output = f&#39;era5_{varkey}_{dt.datetime.strftime(ds[varkey].time[0],&#34;%Y-%m&#34;)}.nc&#39;
                    #TODO    fig.savefig(file_out)
    
    
    #CMEMS
    if model == &#39;CMEMS&#39;:
        date_min_cmems = pd.Timestamp(date_min)-pd.Timedelta(days=1) #CMEMS has daily noon values (not midnight), so subtract one day from date_min to cover desired time extent
        for varkey in varlist:
            Path(dir_output).mkdir(parents=True, exist_ok=True)
            #TODO: update CMEMS urls after 15 april, since some are being replaced
            if varkey in [&#39;bottomT&#39;,&#39;mlotst&#39;,&#39;siconc&#39;,&#39;sithick&#39;,&#39;so&#39;,&#39;thetao&#39;,&#39;uo&#39;,&#39;usi&#39;,&#39;vo&#39;,&#39;vsi&#39;,&#39;zos&#39;]: #for physchem
                #reanalisys: https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description
                #dataset_url = &#39;https://my.cmems-du.eu/thredds/dodsC/cmems_mod_glo_phy_my_0.083_P1D-m&#39;
                #forecast: https://data.marine.copernicus.eu/product/GLOBAL_ANALYSISFORECAST_PHY_001_024/description
                dataset_url = &#39;https://nrt.cmems-du.eu/thredds/dodsC/global-analysis-forecast-phy-001-024&#39; #old location for forecasts (available up to 15 april)
                #dataset_url = &#39;https://nrt.cmems-du.eu/thredds/dodsC/cmems_mod_glo_phy_anfc_0.083deg_PT1H-m.html&#39; #hourly for cur/tem/sal (surface only)
                #dataset_url = &#39;https://nrt.cmems-du.eu/thredds/dodsC/cmems_mod_glo_phy-cur_anfc_0.083deg_P1D-m.html&#39; #currents
                #dataset_url = &#39;https://nrt.cmems-du.eu/thredds/dodsC/cmems_mod_glo_phy-thetao_anfc_0.083deg_P1D-m.html&#39; #temperature
                #dataset_url = &#39;https://nrt.cmems-du.eu/thredds/dodsC/cmems_mod_glo_phy-so_anfc_0.083deg_P1D-m.html&#39; #salinity
            else: # for bio #https://data.marine.copernicus.eu/product/GLOBAL_ANALYSIS_FORECAST_BIO_001_028/description
                dataset_url = &#39;https://my.cmems-du.eu/thredds/dodsC/cmems_mod_glo_bgc_my_0.25_P1D-m&#39; #contains [&#39;chl&#39;,&#39;no3&#39;,&#39;nppv&#39;,&#39;o2&#39;,&#39;po4&#39;,&#39;si&#39;]
                #dataset_url = &#39;https://nrt.cmems-du.eu/thredds/dodsC/global-analysis-forecast-bio-001-028-daily&#39; #contains [&#39;chl&#39;,&#39;fe&#39;,&#39;no3&#39;,&#39;nppv&#39;,&#39;o2&#39;,&#39;ph&#39;,&#39;phyc&#39;,&#39;po4&#39;,&#39;si&#39;,&#39;spco2&#39;]
            file_prefix = &#39;cmems_&#39;
            
            dfmt.download_OPeNDAP(dataset_url=dataset_url,
                                  credentials=None, #credentials=[&#39;username&#39;,&#39;password&#39;], or create &#34;%USERPROFILE%/CMEMS_credentials.txt&#34; with username on line 1 and password on line 2. Register at: https://resources.marine.copernicus.eu/registration-form&#39;
                                  varkey=varkey,
                                  longitude_min=longitude_min-1/12, longitude_max=longitude_max+1/12, latitude_min=latitude_min-1/12, latitude_max=latitude_max+1/12, # download 1 grid cell row/column extra
                                  date_min=date_min_cmems, date_max=date_max,
                                  dir_output=dir_output, file_prefix=file_prefix, overwrite=overwrite)
            
            #open mfdataset to check folder contents and plot first field of each variable
            if make_figs:
                ds = xr.open_mfdataset(os.path.join(dir_output,f&#39;{file_prefix}{varkey}_*.nc&#39;))
                fig,ax = plt.subplots()
                if &#39;depth&#39; in ds[varkey].dims:
                    ds[varkey].isel(time=0,depth=0).plot(ax=ax)
                else:
                    ds[varkey].isel(time=0).plot(ax=ax)
                ds.close()
                ctx.add_basemap(ax=ax,crs=&#34;EPSG:4326&#34;,attribution=False)
        
    
    #HYCOM
    if model == &#39;HYCOM&#39;:
        for varkey in varlist:
            Path(dir_output).mkdir(parents=True, exist_ok=True)
            
            period_range_years = pd.period_range(date_min,date_max,freq=&#39;Y&#39;)
            dataset_url = [f&#39;https://tds.hycom.org/thredds/dodsC/GLBu0.08/expt_19.1/{year}&#39; for year in period_range_years] #list is possible with hycom, since it uses xr.open_mfdataset()
            file_prefix = &#39;hycom_&#39;
            
            dfmt.download_OPeNDAP(dataset_url=dataset_url,
                                  varkey=varkey,
                                  longitude_min=longitude_min, longitude_max=longitude_max, latitude_min=latitude_min, latitude_max=latitude_max,
                                  date_min=date_min, date_max=date_max,
                                  dir_output=dir_output, file_prefix=file_prefix, overwrite=overwrite)
            
            #open mfdataset to check folder contents and plot first field of each variable
            if make_figs:
                ds = xr.open_mfdataset(os.path.join(dir_output,f&#39;{file_prefix}{varkey}_*.nc&#39;))
                fig,ax = plt.subplots()
                if &#39;depth&#39; in ds[varkey].dims:
                    ds[varkey].isel(time=0,depth=0).plot(ax=ax)
                else:
                    ds[varkey].isel(time=0).plot(ax=ax)
                ds.close()
                ctx.add_basemap(ax=ax,crs=&#34;EPSG:4326&#34;,attribution=False)</code></pre>
</details>
</dd>
<dt id="dfm_tools.modelbuilder.preprocess_interpolate_nc_to_bc"><code class="name flex">
<span>def <span class="ident">preprocess_interpolate_nc_to_bc</span></span>(<span>ext_bnd, refdate_str='minutes since 2000-01-01 00:00:00 +00:00', dir_output='./interpolate_nc_to_bc', list_quantities=['waterlevelbnd', 'salinitybnd', 'temperaturebnd', 'uxuy', 'tide'], model='CMEMS', tstart='2012-01-01 12:00', tstop='2012-01-02 12:00', list_plifiles=['p:\\11208054-004-dcsm-fm\\models\\model_input\\bnd_cond\\pli\\DCSM-FM_OB_all_20181108_nocomments.pli'], dir_sourcefiles_hydro='p:\\1204257-dcsmzuno\\data\\CMEMS\\nc\\DCSM_allAvailableTimes', make_figs=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_interpolate_nc_to_bc(
        ext_bnd,
        refdate_str = &#39;minutes since 2000-01-01 00:00:00 +00:00&#39;, # if None, xarray uses ds.time.encoding[&#39;units&#39;] as refdate_str
        dir_output = &#39;./interpolate_nc_to_bc&#39;,
        #quantities should be in conversion_dict.keys(). waterlevelbnd is steric/zos, tide is tidal components from FES/EOT
        list_quantities = [&#39;waterlevelbnd&#39;,&#39;salinitybnd&#39;,&#39;temperaturebnd&#39;,&#39;uxuy&#39;,&#39;tide&#39;], # e.g. [&#39;waterlevelbnd&#39;,&#39;salinitybnd&#39;,&#39;temperaturebnd&#39;,&#39;uxuy&#39;,&#39;tide&#39;,&#39;tracerbndNO3&#39;,&#39;tracerbndOpal&#39;,&#39;tracerbndDON&#39;]
        model = &#39;CMEMS&#39;, #CMEMS GFDL CMCC HYCOM
        tstart = &#39;2012-01-01 12:00&#39;,
        tstop = &#39;2012-01-02 12:00&#39;,
        list_plifiles = [r&#39;p:\11208054-004-dcsm-fm\models\model_input\bnd_cond\pli\DCSM-FM_OB_all_20181108_nocomments.pli&#39;], #TODO: reading this file without &#39;_nocomments&#39; results in empty Polyfile, should raise an error. https://github.com/Deltares/HYDROLIB-core/issues/320
        dir_sourcefiles_hydro = r&#39;p:\1204257-dcsmzuno\data\CMEMS\nc\DCSM_allAvailableTimes&#39;, #CMEMS hydro: bottomT, so, thetao, uo, vo, zos (2012-01-06 12:00:00 to 2013-01-03 12:00:00) (daily values at noon, not at midnight)
        make_figs = True): 
    
    list_plifiles = [Path(x) for x in list_plifiles]
    
    #TODO: add coordinate conversion of pli-coordinates? (for nesting RD models in oceanmodels)
    #TODO: additional models/sources for download/interpolate (evt xESMF for CMCC, climate forcing cmip6 procedure (=calendarconversion) and others)
    
    #The {ncvarname} wildcard in dir_pattern_hydro/dir_patern_waq is used to replace it with conversion_dict[quantity][&#39;ncvarname&#39;] by using str(dir_pattern).format(ncvarname)
    if model==&#39;CMEMS&#39;: #2012-01-06 12:00:00 to 2013-01-03 12:00:00
        conversion_dict = dfmt.get_conversion_dict()
        dir_pattern_hydro = Path(dir_sourcefiles_hydro,&#39;cmems_{ncvarname}_*.nc&#39;) # later remove 2012 from string, but this is faster for testing #TODO: it is quite slow, maybe speed up possible?
        dir_sourcefiles_waq = r&#39;p:\11206304-futuremares\python_scripts\ocean_boundaryCMEMS\data_monthly&#39; #CMEMS waq: no3, o2, phyc, so4, si (2011-12-16 12:00:00 to 2019-01-16 12:00:00)
        dir_pattern_waq = Path(dir_sourcefiles_waq,&#39;cmems_mod_glo_bgc_my_0.25_P1M-m_{ncvarname}_*.nc&#39;) 
        #to reproduce old CMEMS data (icw reverse_depth=True) (from p:\1204257-dcsmzuno\data\CMEMS\bnd\NorthSeaAndBaltic_1993-2019_20210510)
        #tstart = dt.datetime(1993,1,1,12,0)
        #tstop = tstart + dt.timedelta(days=5)
        #dir_pattern_hydro = Path(dir_sourcefiles_hydro,&#39;{ncvarname}_1993*.nc&#39;)
        #TODO tstart = pd.Timestamp(tstart)-pd.Timedelta(days=1) #similar as download: CMEMS has daily noon values (not midnight), so subtract one day from date_min to cover desired time extent
    elif model==&#39;GFDL&#39;:
        conversion_dict = dfmt.get_conversion_dict()
        tstart = &#39;2012-01-16 12:00&#39;
        tstop = &#39;2012-04-01 12:00&#39;
        list_plifiles = [Path(r&#39;p:\11208054-004-dcsm-fm\models\model_input\bnd_cond\pli\DCSM-FM_OB_all_20181108_nocomments.pli&#39;)] #TODO: reading this file without &#39;_nocomments&#39; results in empty Polyfile, should raise an error. https://github.com/Deltares/HYDROLIB-core/issues/320
        dir_sourcefiles_hydro = None
        dir_pattern_hydro = None
        dir_sourcefiles_waq = r&#39;p:\11206304-futuremares\data\CMIP6_BC\GFDL-ESM4&#39; #GFDL waq: no3 (1850-01-16 12:00:00 to 2014-12-16 12:00:00)
        dir_pattern_waq = Path(dir_sourcefiles_waq,&#39;{ncvarname}_esm-hist.nc&#39;)
    elif model==&#39;CMCC&#39;: #TODO: check method, now finding nearest points (so always has values)
        #TODO: time_bnds/lev_bnds are available, take into account in bc file?
        conversion_dict = dfmt.get_conversion_dict(ncvarname_updates={&#39;salinitybnd&#39;:&#39;sos&#39;, &#39;temperaturebnd&#39;:&#39;tos&#39;})
        conversion_dict[&#39;tracerbndNO3&#39;] = {&#39;ncvarname&#39;:&#39;no3&#39;, &#39;unit&#39;:&#39;g/m3&#39;, &#39;conversion&#39;:14.0} #other vars also have different conversion than cmems
        tstart = &#39;2015-06-16 12:00&#39;
        tstop = &#39;2015-12-01 12:00&#39;
        list_plifiles = [Path(r&#39;p:\11208054-004-dcsm-fm\models\model_input\bnd_cond\pli\DCSM-FM_OB_all_20181108_nocomments.pli&#39;)] #TODO: reading this file without &#39;_nocomments&#39; results in empty Polyfile, should raise an error. https://github.com/Deltares/HYDROLIB-core/issues/320
        dir_sourcefiles_hydro = r&#39;p:\11206304-futuremares\data\CMIP6_BC\CMCC-ESM2&#39;
        dir_pattern_hydro = Path(dir_sourcefiles_hydro,&#39;{ncvarname}_Omon_CMCC-ESM2_ssp126_r1i1p1f1_gn_*.nc&#39;)
        dir_sourcefiles_waq = dir_sourcefiles_hydro #CMCC waq: (2015-01-16 12:00:00 to 2100-12-16 12:00:00)
        dir_pattern_waq = dir_pattern_hydro
    elif model==&#39;HYCOM&#39;:
        conversion_dict = dfmt.get_conversion_dict(ncvarname_updates={&#39;salinitybnd&#39;:&#39;salinity&#39;, &#39;temperaturebnd&#39;:&#39;water_temp&#39;})
        tstart = &#39;2016-04-20&#39;
        tstop = &#39;2016-05-03&#39;
        list_plifiles = [Path(r&#39;c:\DATA\dfm_tools_testdata\GLBu0.08_expt_91.2\bcline.pli&#39;)] #HYCOM not available in DCSM area, so use other pli-file
        dir_sourcefiles_hydro = &#39;c:\\DATA\\dfm_tools_testdata\\GLBu0.08_expt_91.2&#39; #HYCOM hydro: salinity/so, water_temp/thetao (2016-04-19 00:00:00 to 2016-05-06 00:00:00)
        dir_pattern_hydro = Path(dir_sourcefiles_hydro,&#39;HYCOM_ST_GoO_*.nc&#39;)
        dir_sourcefiles_waq = None
        dir_pattern_waq = None
    else:
        raise Exception(f&#39;invalid model: {model}&#39;)
    
    # start of interpolation process
    dtstart = dt.datetime.now()
    if not os.path.isdir(dir_output):
        os.mkdir(dir_output)
    
    for file_pli in list_plifiles:
        file_bc_basename = file_pli.name.replace(&#39;.pli&#39;,&#39;&#39;)
        for quantity in list_quantities:
            print(f&#39;processing quantity: {quantity}&#39;)
            if quantity==&#39;tide&#39;: 
                tidemodel = &#39;FES2014&#39; #FES2014, FES2012, EOT20, GTSM4.1preliminary
                if tidemodel == &#39;FES2014&#39;: #for comparing to older FES bc-files #TODO: choose flexible/generic component notation
                    component_list = [&#39;2n2&#39;,&#39;mf&#39;,&#39;p1&#39;,&#39;m2&#39;,&#39;mks2&#39;,&#39;mu2&#39;,&#39;q1&#39;,&#39;t2&#39;,&#39;j1&#39;,&#39;m3&#39;,&#39;mm&#39;,&#39;n2&#39;,&#39;r2&#39;,&#39;k1&#39;,&#39;m4&#39;,&#39;mn4&#39;,&#39;s1&#39;,&#39;k2&#39;,&#39;m6&#39;,&#39;ms4&#39;,&#39;nu2&#39;,&#39;s2&#39;,&#39;l2&#39;,&#39;m8&#39;,&#39;msf&#39;,&#39;o1&#39;,&#39;s4&#39;]
                else:
                    component_list = None #None results in all tidemodel components
                ForcingModel_object = dfmt.interpolate_tide_to_bc(tidemodel=tidemodel, file_pli=file_pli, component_list=component_list, nPoints=None)
                for forcingobject in ForcingModel_object.forcing: #add A0 component
                    forcingobject.datablock.append([&#39;A0&#39;,0.0,0.0])
            else:
                if quantity in [&#39;waterlevelbnd&#39;,&#39;salinitybnd&#39;,&#39;temperaturebnd&#39;,&#39;uxuy&#39;]: #hydro
                    if dir_sourcefiles_hydro is None:
                        continue
                    if (model==&#39;HYCOM&#39;) &amp; (quantity not in [&#39;salinitybnd&#39;,&#39;temperaturebnd&#39;]): #only contains quantities salinity and water_temp, so crashes on others
                        continue
                    dir_pattern = dir_pattern_hydro
                else: #waq
                    if dir_pattern_waq is None:
                        continue
                    dir_pattern = dir_pattern_waq
                
                #open regulargridDataset and do some basic stuff (time selection, renaming depth/lat/lon/varname, converting units, etc)
                data_xr_vars = dfmt.open_dataset_extra(dir_pattern=dir_pattern, quantity=quantity, #TODO: maybe replace renaming part with package CMCC/Lisa?
                                                       tstart=tstart, tstop=tstop,
                                                       conversion_dict=conversion_dict,
                                                       refdate_str=refdate_str)
                #interpolate regulargridDataset to plipointsDataset
                data_interp = dfmt.interp_regularnc_to_plipoints(data_xr_reg=data_xr_vars, file_pli=file_pli, #TODO: difference in .interp() with float vs da arguments: https://github.com/Deltares/dfm_tools/issues/287
                                                                 nPoints=None) #argument for testing
                
                #convert plipointsDataset to hydrolib ForcingModel
                ForcingModel_object = dfmt.plipointsDataset_to_ForcingModel(plipointsDataset=data_interp)
                        
            file_bc_basename = file_pli.name.replace(&#39;.pli&#39;,&#39;&#39;)
            if quantity==&#39;tide&#39;:
                file_bc_out = Path(dir_output,f&#39;{quantity}_{file_bc_basename}_{tidemodel}.bc&#39;)
            else:
                file_bc_out = Path(dir_output,f&#39;{quantity}_{file_bc_basename}_{model}.bc&#39;)
            
            print(f&#39;writing ForcingModel to bc file with hydrolib ({file_bc_out.name})&#39;)
            bc_type = &#39;bc&#39; #TODO: add netcdf bc support. https://github.com/Deltares/HYDROLIB-core/issues/318
            if bc_type==&#39;bc&#39;:
                #ForcingModel_object.serializer_config.float_format = &#39;.3f&#39; #TODO SOLVED: improve formatting of bc file: https://github.com/Deltares/HYDROLIB-core/issues/308
                #ForcingModel_object.serializer_config.float_format_datablock = &#39;.5f&#39; #maybe move this to interp_regularnc_to_plipoints/interpolate_tide_to_bc?
                ForcingModel_object.save(filepath=file_bc_out)
            
            #TODO: support for relative paths?
            #generate boundary object for the ext file (quantity, pli-filename, bc-filename)
            boundary_object = hcdfm.Boundary(quantity=quantity.replace(&#39;tide&#39;,&#39;waterlevelbnd&#39;), #the FM quantity for tide is also waterlevelbnd
                                             locationfile=file_pli,
                                             forcingfile=ForcingModel_object)
            ext_bnd.boundary.append(boundary_object)
    
            if make_figs and quantity!=&#39;tide&#39;: #TODO: data_xr_vars/data_interp does not exist for tide yet
                #plotting dataset and polyline (is wrong for CMCC)
                varname0 = list(data_xr_vars.data_vars)[0] 
                fig,ax = plt.subplots()
                if &#39;depth&#39; in data_xr_vars[varname0].dims:
                    data_xr_vars[varname0].isel(time=0,depth=0).plot(ax=ax)
                else:
                    data_xr_vars[varname0].isel(time=0).plot(ax=ax)
                plipoint_coords = data_interp.plipoints.to_dataframe()
                ax.plot(plipoint_coords[&#39;plipoint_x&#39;],plipoint_coords[&#39;plipoint_y&#39;],&#39;r-&#39;)
                ctx.add_basemap(ax=ax,crs=&#34;EPSG:4326&#34;,attribution=False)
                fig.tight_layout()
                fig.savefig(str(file_bc_out).replace(&#39;.bc&#39;,&#39;_polyline&#39;))
                
                #plotting example data point
                for iF in [2]:#range(nPoints): 
                    data_vars = list(data_interp.data_vars)
                    fig,ax1 = plt.subplots(figsize=(10, 6))
                    data_interp[data_vars[0]].isel(plipoints=iF).T.plot()
                    fig.tight_layout()
                    fig.savefig(str(file_bc_out).replace(&#39;.bc&#39;,&#39;&#39;))
    
    #file_ext_out = Path(dir_output,&#39;example_bnd.ext&#39;)
    #ext_bnd.save(filepath=file_ext_out)
    
    time_passed = (dt.datetime.now()-dtstart).total_seconds()
    print(f&#39;&gt;&gt;total script time passed: {time_passed:.2f} sec&#39;)
    return ext_bnd</code></pre>
</details>
</dd>
<dt id="dfm_tools.modelbuilder.preprocess_ini_cmems_to_nc"><code class="name flex">
<span>def <span class="ident">preprocess_ini_cmems_to_nc</span></span>(<span>tSimStart=datetime.datetime(1998, 1, 1, 0, 0), dir_data='p:\\i1000668-tcoms\\03_newModel\\01_input\\02_bnd\\data_opendap', dir_out='.')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_ini_cmems_to_nc(tSimStart = dt.datetime(1998,1,1),
        dir_data  = r&#39;p:\i1000668-tcoms\03_newModel\01_input\02_bnd\data_opendap&#39;, #folder containing CMEMS so and thetao netcdf files
        dir_out = &#39;.&#39;):
    #TODO: merge with other ini script and make generic for getting an inifield out of CMEMS/etc regulargrid Dataset or a 2D/3D FM map/rst Dataset
    
    file_nc_list_so = glob.glob(f&#39;{dir_data}\\cmems_so_*.nc&#39;)
    file_nc_list_thetao = glob.glob(f&#39;{dir_data}\\cmems_thetao_*.nc&#39;)
    file_nc_list = file_nc_list_so + file_nc_list_thetao
    
    print(f&#39;opening {len(file_nc_list)} datasets&#39;)
    data_xr = xr.open_mfdataset(file_nc_list)
    
    if 0: #this would be the proper way to do it, but FM needs two timesteps for some reason
        print(&#39;ds.interp()&#39;)
        data_xr_ontime = data_xr.interp(time=[tSimStart],kwargs=dict(bounds_error=True)) #bounds_error makes sure, outofbounds time results in &#34;ValueError: A value in x_new is below the interpolation range.&#34;
    else:
        print(&#39;ds.sel()&#39;)
        data_xr_ontime = data_xr.sel(time=slice(tSimStart-dt.timedelta(days=1),tSimStart+dt.timedelta(days=1)))
    
    print(&#39;writing file&#39;)
    outFile = os.path.join(dir_out,f&#39;InitialField_{tSimStart.strftime(&#34;%Y-%m-%d_%H-%M-%S&#34;)}.nc&#39;)
    data_xr_ontime.to_netcdf(outFile,format=&#34;NETCDF4_CLASSIC&#34;)</code></pre>
</details>
</dd>
<dt id="dfm_tools.modelbuilder.preprocess_merge_meteofiles"><code class="name flex">
<span>def <span class="ident">preprocess_merge_meteofiles</span></span>(<span>mode='HYCOM', varkey_list=[], dir_data=[], dir_output='.', time_slice=slice('2013-12-30', '2014-01-01', None))</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_merge_meteofiles(
        mode = &#39;HYCOM&#39;, # &#39;HIRLAM_meteo&#39; &#39;HIRLAM_meteo-heatflux&#39; &#39;HARMONIE&#39; &#39;HYCOM&#39; &#39;ERA5_wind_pressure&#39; &#39;ERA5_heat_model&#39; &#39;ERA5_radiation&#39; &#39;ERA5_rainfall&#39; &#39;WOA&#39;
        varkey_list = [],
        dir_data = [],
        dir_output = &#39;.&#39;,
        time_slice = slice(&#39;2013-12-30&#39;,&#39;2014-01-01&#39;)
        ):

    if isinstance(varkey_list[0], list):
        varkey_lists = varkey_list
    else:
        varkey_list = [varkey_list]
        
    for varkey_list in varkey_lists:
        if &#39;HIRLAM&#39; in mode:
            if mode == &#39;HIRLAM_meteo&#39;: #1year voor meteo crasht (HIRLAM72_*\\h72_*) door conflicting dimension sizes, sourcefolders opruimen? meteo_heatflux folders zijn schoner dus daar werkt het wel
                dir_data = &#39;P:\\1204257-dcsmzuno\\2014\\data\\meteo\\HIRLAM72_*&#39; #files contain: [&#39;air_pressure_fixed_height&#39;,&#39;northward_wind&#39;,&#39;eastward_wind&#39;]
            elif mode == &#39;HIRLAM_meteo-heatflux&#39;:
                dir_data = &#39;P:\\1204257-dcsmzuno\\2014\\data\\meteo-heatflux\\HIRLAM72_*&#39; # files contain: [&#39;dew_point_temperature&#39;,&#39;air_temperature&#39;,&#39;cloud_area_fraction&#39;]
            fn_match_pattern = &#39;h72_20131*.nc&#39;
            file_out_prefix = &#39;h72_&#39;
            preprocess = dfmt.preprocess_hirlam #temporary(?) fix for &gt;1D-vars with same name as its dim
        elif mode == &#39;HARMONIE&#39;:
            dir_data = &#39;p:\\1204257-dcsmzuno\\data\\meteo\\HARMONIE\\nc\\air_*&#39; #many invalid files, so subsetting here
            fn_match_pattern = &#39;HARMONIE_*_2020_*.nc&#39;
            file_out_prefix = &#39;HARMONIE_&#39;
            preprocess = None
        elif mode == &#39;HYCOM&#39;:
            dir_data = &#39;c:\\DATA\\dfm_tools_testdata\\GLBu0.08_expt_91.2&#39;
            fn_match_pattern = &#39;HYCOM_ST_GoO_*.nc&#39;
            file_out_prefix = &#39;HYCOM_ST_GoO_&#39;
            preprocess = None
            #rename_variables = {&#39;salinity&#39;:&#39;so&#39;, &#39;water_temp&#39;:&#39;thetao&#39;}
        elif &#39;ERA5&#39; in mode:
            fn_match_pattern = f&#39;era5_.*({&#34;|&#34;.join(varkey_list)})_.*.nc&#39; #simpler but selects more files: &#39;era5_*.nc&#39;
            file_out_prefix = f&#39;era5_{&#34;_&#34;.join(varkey_list)}_&#39;
            preprocess = dfmt.preprocess_ERA5 #reduce expver dimension if present
        elif mode == &#39;WOA&#39;:
            dir_data = r&#39;p:\1204257-dcsmzuno\data\WOA13&#39;
            fn_match_pattern = &#39;woa13_decav_s*.nc&#39;
            file_out_prefix = &#39;woa13_decav_s_&#39;
            preprocess = dfmt.preprocess_woa #add 360-day calendar unit to time attrs before decode_cf
        else:
            raise Exception(&#39;ERROR: wrong mode %s&#39;%(mode))
        
        if not os.path.exists(dir_output):
            os.makedirs(dir_output)
        
        file_nc = os.path.join(dir_data,fn_match_pattern)
        
        data_xr_tsel = dfmt.merge_meteofiles(file_nc=file_nc, time_slice=time_slice, 
                                             preprocess=preprocess,
                                             add_global_overlap=False, #GTSM specific: extend data beyond -180 to 180 longitude
                                             zerostart=False) #GTSM specific: extend data with 0-value fields 1 and 2 days before all_tstart
        
        #write to netcdf file
        print(&#39;&gt;&gt; writing file (can take a while): &#39;,end=&#39;&#39;)
        dtstart = dt.datetime.now()
        try:
            times_np = data_xr_tsel[&#39;time&#39;].to_series()
        except:
            times_np = data_xr_tsel[&#39;time&#39;].to_numpy() #.to_series() does not work for woa 360_day data. .to_numpy() results in numpy.datetime64 for other datasets, which has no attribute strftime
        time_start_str = times_np[0].strftime(&#34;%Y%m%d&#34;)
        time_stop_str = times_np[-1].strftime(&#34;%Y%m%d&#34;)
        file_out = os.path.join(dir_output, f&#39;{file_out_prefix}{time_start_str}to{time_stop_str}_{mode}.nc&#39;)
        data_xr_tsel.to_netcdf(file_out)
        print(f&#39;{(dt.datetime.now()-dtstart).total_seconds():.2f} sec&#39;)
        
        
        #load outputfile
        data_xr_check = xr.open_dataset(file_out)
        
        for varkey in data_xr_check.data_vars:
            varsel = data_xr_check[varkey]
            if not set([&#39;longitude&#39;,&#39;latitude&#39;]).issubset(set(varsel.coords)): #skipping vars without lat/lon coordinate
                continue
            print(f&#39;plotting {varkey}&#39;)
            fig,ax1 = plt.subplots()
            if &#39;HIRLAM&#39; in mode:
                varsel.isel(time=0).plot(ax=ax1,x=&#39;longitude&#39;,y=&#39;latitude&#39;) #x/y are necessary since coords are not 1D and dims
            elif &#39;depth&#39; in data_xr_tsel[varkey].coords:
                varsel.isel(time=0).sel(depth=0).plot(ax=ax1)
            else:
                varsel.isel(time=0).plot(ax=ax1)
            file_out = os.path.join(dir_output, f&#39;era5_{varkey}_{time_start_str}&#39;)
            fig.savefig(file_out)</code></pre>
</details>
</dd>
<dt id="dfm_tools.modelbuilder.make_basegrid"><code class="name flex">
<span>def <span class="ident">make_basegrid</span></span>(<span>lon_min, lon_max, lat_min, lat_max, dx=0.05, dy=0.05, angle=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_basegrid(lon_min,lon_max,lat_min,lat_max,dx=0.05,dy=0.05,angle=0):
    print(&#39;modelbuilder.make_basegrid()&#39;)
    # create base grid
    nox = int(np.round((lon_max-lon_min)/dx))
    noy = int(np.round((lat_max-lat_min)/dy))
    
    make_grid_parameters = meshkernel.MakeGridParameters(num_columns=nox,
                                                         num_rows=noy,
                                                         angle=angle,
                                                         origin_x=lon_min,
                                                         origin_y=lat_min,
                                                         block_size_x=dx,
                                                         block_size_y=dy)
    #print(make_grid_parameters)
    
    geometry_list = meshkernel.GeometryList(np.empty(0, dtype=np.double), np.empty(0, dtype=np.double)) # A polygon must to be provided. If empty it will not be used. If a polygon is provided it will be used in the generation of the curvilinear grid. The polygon must be closed
    mk = meshkernel.MeshKernel() #TODO: is_geographic=True was used in modelbuilder, but refinement super slow and raises &#34;MeshKernelError: MeshRefinement::connect_hanging_nodes: The number of non-hanging nodes is neither 3 nor 4.&#34;
    mk.curvilinear_make_uniform(make_grid_parameters, geometry_list) #TODO: make geometry_list argument optional: https://github.com/Deltares/MeshKernelPy/issues/30
    mk.curvilinear_convert_to_mesh2d() #convert to ugrid/mesh2d
    
    #plot
    mesh2d_mesh_kernel = mk.mesh2d_get()
    fig, ax = plt.subplots()
    mesh2d_mesh_kernel.plot_edges(ax, color=&#39;blue&#39;)
    source = ctx.providers.Esri.WorldImagery
    ctx.add_basemap(ax=ax, source=source, crs=&#39;EPSG:4326&#39;, attribution=False)
    
    return mk</code></pre>
</details>
</dd>
<dt id="dfm_tools.modelbuilder.refine_basegrid"><code class="name flex">
<span>def <span class="ident">refine_basegrid</span></span>(<span>mk, data_bathy_sel, min_face_size=0.1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refine_basegrid(mk, data_bathy_sel,min_face_size=0.1):
    print(&#39;modelbuilder.refine_basegrid()&#39;)
    samp_x,samp_y = np.meshgrid(data_bathy_sel.lon.to_numpy(),data_bathy_sel.lat.to_numpy())
    samp_z = data_bathy_sel.elevation.to_numpy().astype(float) #TODO: without .astype(float), meshkernelpy generates &#34;TypeError: incompatible types, c_short_Array_27120 instance instead of LP_c_double instance&#34;: https://github.com/Deltares/MeshKernelPy/issues/31
    samp_x = samp_x.ravel()
    samp_y = samp_y.ravel()
    samp_z = samp_z.ravel()
    geomlist = meshkernel.GeometryList(x_coordinates=samp_x, y_coordinates=samp_y, values=samp_z) #TODO: does not check if lenghts of input array is equal (samp_z[1:]) https://github.com/Deltares/MeshKernelPy/issues/32
    
    #refinement
    mesh_refinement_parameters = meshkernel.MeshRefinementParameters(refine_intersected=False, #TODO: provide defaults for several arguments, so less arguments are required
                                                                     use_mass_center_when_refining=False, #TODO: what does this do?
                                                                     min_face_size=min_face_size, #TODO: size in meters would be more convenient: https://github.com/Deltares/MeshKernelPy/issues/33
                                                                     refinement_type=meshkernel.RefinementType(1), #Wavecourant/1,
                                                                     connect_hanging_nodes=True, #set to False to do multiple refinement steps (e.g. for multiple regions)
                                                                     account_for_samples_outside_face=True, #outsidecell argument for --refine?
                                                                     max_refinement_iterations=5,
                                                                     ) #TODO: missing the arguments dtmax (necessary?), hmin (min_face_size but then in meters instead of degrees), smoothiters (currently refinement is patchy along coastlines, goes good in dflowfm exec after additional implementation of HK), spherical 1/0 (necessary?)
    
    mk.mesh2d_refine_based_on_samples(samples=geomlist,
                                       relative_search_radius=0.5, #TODO: bilin interp is preferred, but this is currently not supported (samples have to be ravelled): https://github.com/Deltares/MeshKernelPy/issues/34
                                       minimum_num_samples=3,
                                       mesh_refinement_params=mesh_refinement_parameters,
                                       )
    
    #plotting
    mesh2d_grid2 = mk.mesh2d_get()
    fig, ax = plt.subplots()
    mesh2d_grid2.plot_edges(ax,linewidth=1.2)
    ctx.add_basemap(ax=ax, crs=&#39;EPSG:4326&#39;, attribution=False)
    
    return mk</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dfm_tools" href="index.html">dfm_tools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="dfm_tools.modelbuilder.download_meteodata_oceandata" href="#dfm_tools.modelbuilder.download_meteodata_oceandata">download_meteodata_oceandata</a></code></li>
<li><code><a title="dfm_tools.modelbuilder.preprocess_interpolate_nc_to_bc" href="#dfm_tools.modelbuilder.preprocess_interpolate_nc_to_bc">preprocess_interpolate_nc_to_bc</a></code></li>
<li><code><a title="dfm_tools.modelbuilder.preprocess_ini_cmems_to_nc" href="#dfm_tools.modelbuilder.preprocess_ini_cmems_to_nc">preprocess_ini_cmems_to_nc</a></code></li>
<li><code><a title="dfm_tools.modelbuilder.preprocess_merge_meteofiles" href="#dfm_tools.modelbuilder.preprocess_merge_meteofiles">preprocess_merge_meteofiles</a></code></li>
<li><code><a title="dfm_tools.modelbuilder.make_basegrid" href="#dfm_tools.modelbuilder.make_basegrid">make_basegrid</a></code></li>
<li><code><a title="dfm_tools.modelbuilder.refine_basegrid" href="#dfm_tools.modelbuilder.refine_basegrid">refine_basegrid</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>