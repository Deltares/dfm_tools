<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dfm_tools.xarray_helpers API documentation</title>
<meta name="description" content="Created on Fri Oct 14 19:58:36 2022 â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dfm_tools.xarray_helpers</code></h1>
</header>
<section id="section-intro">
<p>Created on Fri Oct 14 19:58:36 2022</p>
<p>@author: veenstra</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
Created on Fri Oct 14 19:58:36 2022

@author: veenstra
&#34;&#34;&#34;

import os
import re
from netCDF4 import Dataset
import xarray as xr
import xugrid as xu
import matplotlib.pyplot as plt
plt.close(&#39;all&#39;)
import datetime as dt
import glob
import pandas as pd
import warnings
import numpy as np


def file_to_list(file_nc):
    if isinstance(file_nc,list):
        file_nc_list = file_nc
    else:
        basename = os.path.basename(file_nc)
        dirname = os.path.dirname(file_nc)
        ext = os.path.splitext(file_nc)[-1]
        if &#39;.*&#39; in basename:
            def glob_re(pattern, strings):
                return list(filter(re.compile(pattern).search, strings))
            file_nc_list = glob_re(basename, glob.glob(os.path.join(dirname,f&#39;*{ext}&#39;)))
        else:
            file_nc_list = glob.glob(file_nc)
        file_nc_list.sort()
    if len(file_nc_list)==0:
        raise Exception(&#39;file(s) not found, empty file_nc_list&#39;)
    return file_nc_list


def preprocess_hisnc(ds):
    &#34;&#34;&#34;
    Look for dim/coord combination and use this for Dataset.set_index(), to enable station/gs/crs/laterals label based indexing. If duplicate labels are found (like duplicate stations), these are dropped to avoid indexing issues.
    
    Parameters
    ----------
    ds : xarray.Dataset
        DESCRIPTION.
    drop_duplicate_stations : TYPE, optional
        DESCRIPTION. The default is True.

    Returns
    -------
    ds : TYPE
        DESCRIPTION.

    &#34;&#34;&#34;
    
    #generate dim_coord_dict to set indexes, this will be something like {&#39;stations&#39;:&#39;station_name&#39;,&#39;cross_section&#39;:&#39;cross_section_name&#39;} after loop
    dim_coord_dict = {}
    for ds_coord in ds.coords.keys():
        ds_coord_dtype = ds[ds_coord].dtype
        ds_coord_dim = ds[ds_coord].dims[0] #these vars always have only one dim
        if ds_coord_dtype.str.startswith(&#39;|S&#39;): #these are station/crs/laterals/gs names/ids
            dim_coord_dict[ds_coord_dim] = ds_coord
    
    #loop over dimensions and set corresponding coordinates/variables from dim_coord_dict as their index
    for dim in dim_coord_dict.keys():
        coord = dim_coord_dict[dim]
        ds[coord] = ds[coord].load().str.decode(&#39;utf-8&#39;,errors=&#39;ignore&#39;).str.strip() #.load() is essential to convert not only first letter of string.
        ds = ds.set_index({dim:coord})
        
        #drop duplicate indices (stations/crs/gs), this avoids &#34;InvalidIndexError: Reindexing only valid with uniquely valued Index objects&#34;
        duplicated_keepfirst = ds[dim].to_series().duplicated(keep=&#39;first&#39;)
        if duplicated_keepfirst.sum()&gt;0:
            print(f&#39;dropping {duplicated_keepfirst.sum()} duplicate &#34;{coord}&#34; labels to avoid InvalidIndexError&#39;)
            ds = ds[{dim:~duplicated_keepfirst}]

    #check dflowfm version/date and potentially raise warning about incorrect layers
    try:
        source_attr = ds.attrs[&#39;source&#39;] # fails if no source attr present in dataset
        source_attr_version = source_attr.split(&#39;, &#39;)[1]
        source_attr_date = source_attr.split(&#39;, &#39;)[2]
        if pd.Timestamp(source_attr_date) &lt; dt.datetime(2020,11,28):
            warnings.warn(UserWarning(f&#39;Your model was run with a D-FlowFM version from before 28-10-2020 ({source_attr_version} from {source_attr_date}), the layers in the hisfile are incorrect. Check UNST-2920 and UNST-3024 for more information, it was fixed from OSS 67858.&#39;))
    except: #no source attr present in hisfile, cannot check version
        pass

    return ds


def preprocess_hirlam(ds):
    &#34;&#34;&#34;
    add xy variables as longitude/latitude to avoid duplicate var/dim names (we rename it anyway otherwise)
    add xy as variables again with help of NetCDF4
    this function is hopefully temporary, necessary since &gt;1D-variables cannot have the same name as one of its dimensions in xarray. Background and future solution: https://github.com/pydata/xarray/issues/6293
    &#34;MissingDimensionsError: &#39;y&#39; has more than 1-dimension and the same name as one of its dimensions (&#39;y&#39;, &#39;x&#39;). xarray disallows such variables because they conflict with the coordinates used to label dimensions.&#34;
    might be solved in https://github.com/pydata/xarray/releases/tag/v2023.02.0 (xr.concat), but not certain
    &#34;&#34;&#34;
    
    print(&#39;hirlam workaround: adding dropped x/y variables again as lon/lat&#39;)
    file_nc_one = ds.encoding[&#39;source&#39;]
    with Dataset(file_nc_one) as data_nc:
        data_nc_x = data_nc[&#39;x&#39;]
        data_nc_y = data_nc[&#39;y&#39;]
        ds[&#39;longitude&#39;] = xr.DataArray(data_nc_x,dims=data_nc_x.dimensions,attrs=data_nc_x.__dict__) 
        ds[&#39;latitude&#39;] = xr.DataArray(data_nc_y,dims=data_nc_y.dimensions,attrs=data_nc_y.__dict__)
    ds = ds.set_coords([&#39;latitude&#39;,&#39;longitude&#39;])
    for varkey in ds.data_vars:
        del ds[varkey].encoding[&#39;coordinates&#39;] #remove {&#39;coordinates&#39;:&#39;y x&#39;} from encoding (otherwise set twice)
    return ds


def preprocess_ERA5(ds):
    &#34;&#34;&#34;
    Reduces the expver dimension in some of the ERA5 data (mtpr and other variables), which occurs in files with very recent data. The dimension contains the unvalidated data from the latest month in the second index in the expver dimension. The reduction is done with mean, but this is arbitrary, since there is only one valid value per timestep and the other one is nan.
    &#34;&#34;&#34;
    if &#39;expver&#39; in ds.dims:
        ds = ds.mean(dim=&#39;expver&#39;)
    return ds


def preprocess_woa(ds):
    &#34;&#34;&#34;
    WOA time units is &#39;months since 0000-01-01 00:00:00&#39; and calendar is not set (360_day is the only calendar that supports that unit in xarray)
    &#34;&#34;&#34;
    ds.time.attrs[&#39;calendar&#39;] = &#39;360_day&#39;
    ds = xr.decode_cf(ds) #decode_cf after adding 360_day calendar attribute
    return ds


def prevent_dtype_int(ds):
    &#34;&#34;&#34;
    Prevent writing to int, since it might mess up dataset (https://github.com/Deltares/dfm_tools/issues/239)
    &#34;&#34;&#34;
    for var in ds.data_vars:
        var_encoding = ds[var].encoding
        if &#39;dtype&#39; in var_encoding.keys():
            if &#39;int&#39; in str(var_encoding[&#39;dtype&#39;]):
                ds[var].encoding[&#39;dtype&#39;] = np.dtype(&#39;float32&#39;)
    return ds


def merge_meteofiles(file_nc:str, preprocess=None, 
                     chunks:dict = {&#39;time&#39;:1},
                     time_slice:slice = slice(None,None),
                     add_global_overlap:bool = False, zerostart:bool = False) -&gt; xr.Dataset:
    &#34;&#34;&#34;
    for merging for instance meteo files
    x/y and lon/lat are renamed to longitude/latitude #TODO: is this desireable?

    Parameters
    ----------
    file_nc : str
        DESCRIPTION.
    preprocess : TYPE, optional
        DESCRIPTION. The default is None.
    chunks : dict, optional
        Prevents large chunks and memory issues. The default is {&#39;time&#39;:1}.
    time_slice : slice, optional
        DESCRIPTION. The default is slice(None,None).
    add_global_overlap : bool, optional
        GTSM specific: extend data beyond -180 to 180 longitude. The default is False.
    zerostart : bool, optional
        GTSM specific: extend data with 0-value fields 1 and 2 days before time_slice.start. The default is False.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    TYPE
        DESCRIPTION.

    &#34;&#34;&#34;
    #TODO: add ERA5 conversions and features from hydro_tools\ERA5\ERA52DFM.py (except for varRhoair_alt, request FM support for varying airpressure: https://issuetracker.deltares.nl/browse/UNST-6593)
    #TODO: request FM support for charnock (etc) separate meteo forcing (currently airpressure_windx_windy_charnock merged file is required): https://issuetracker.deltares.nl/browse/UNST-6453
    #TODO: provide extfile example with fmquantity/ncvarname combinations and cleanup FM code: https://issuetracker.deltares.nl/browse/UNST-6453
    #TODO: add coordinate conversion (only valid for models with multidimensional lat/lon variables like HARMONIE and HIRLAM). This should work: ds_reproj = ds.set_crs(4326).to_crs(28992)
    #TODO: add CMCC etc from gtsmip repos (mainly calendar conversion)
    #TODO: put conversions in separate function?
    #TODO: maybe add renaming like {&#39;salinity&#39;:&#39;so&#39;, &#39;water_temp&#39;:&#39;thetao&#39;} for hycom
    
    #hirlam workaround
    if preprocess == preprocess_hirlam:
        drop_variables = [&#39;x&#39;,&#39;y&#39;] #will be added again as longitude/latitude, this is a workaround (see dfmt.preprocess_hirlam for details)
    else:
        drop_variables = None
    #woa workaround
    if preprocess == preprocess_woa:
        decode_cf = False
    else:
        decode_cf = True        

    file_nc_list = file_to_list(file_nc)

    print(f&#39;&gt;&gt; opening multifile dataset of {len(file_nc_list)} files (can take a while with lots of files): &#39;,end=&#39;&#39;)
    dtstart = dt.datetime.now()
    data_xr = xr.open_mfdataset(file_nc_list,
                                parallel=True, #speeds up the process
                                preprocess=preprocess,
                                chunks=chunks,
                                drop_variables=drop_variables, #necessary since dims/vars with equal names are not allowed by xarray, add again later and requested matroos to adjust netcdf format.
                                decode_cf=decode_cf)
    print(f&#39;{(dt.datetime.now()-dtstart).total_seconds():.2f} sec&#39;)
    
    #rename variables
    if not &#39;longitude&#39; in data_xr.variables: #TODO: make generic, comparable rename in rename_dims_dict in dfmt.open_dataset_extra()
        if &#39;lon&#39; in data_xr.variables:
            data_xr = data_xr.rename({&#39;lon&#39;:&#39;longitude&#39;, &#39;lat&#39;:&#39;latitude&#39;})
        elif &#39;x&#39; in data_xr.variables:
            data_xr = data_xr.rename({&#39;x&#39;:&#39;longitude&#39;, &#39;y&#39;:&#39;latitude&#39;})
        else:
            raise Exception(&#39;no longitude/latitude, lon/lat or x/y variables found in dataset&#39;)

    varkeys = data_xr.variables.mapping.keys()
    #data_xr.attrs[&#39;comment&#39;] = &#39;merged with dfm_tools from https://github.com/Deltares/dfm_tools&#39; #TODO: add something like this or other attributes? (some might also be dropped now)
    
    #select time and do checks #TODO: check if calendar is standard/gregorian
    print(&#39;time selection&#39;)
    data_xr_tsel = data_xr.sel(time=time_slice)
    if data_xr_tsel.get_index(&#39;time&#39;).duplicated().any():
        print(&#39;dropping duplicate timesteps&#39;)
        data_xr_tsel = data_xr_tsel.sel(time=~data_xr_tsel.get_index(&#39;time&#39;).duplicated()) #drop duplicate timesteps
    
    #check if there are times selected
    if len(data_xr_tsel.time)==0:
        raise Exception(f&#39;ERROR: no times selected, ds_text={data_xr.time[[0,-1]].to_numpy()} and time_slice={time_slice}&#39;)
    
    #check if there are no gaps (more than one unique timestep)
    times_pd = data_xr_tsel[&#39;time&#39;].to_series()
    timesteps_uniq = times_pd.diff().iloc[1:].unique()
    if len(timesteps_uniq)&gt;1:
        raise Exception(f&#39;ERROR: gaps found in selected dataset (are there sourcefiles missing?), unique timesteps (hour): {timesteps_uniq/1e9/3600}&#39;)
    
    #check if requested times are available in selected files (in times_pd)
    if not time_slice.start in times_pd.index:
        raise Exception(f&#39;ERROR: time_slice_start=&#34;{time_slice.start}&#34; not in selected files, timerange: &#34;{times_pd.index[0]}&#34; to &#34;{times_pd.index[-1]}&#34;&#39;)
    if not time_slice.stop in times_pd.index:
        raise Exception(f&#39;ERROR: time_slice_stop=&#34;{time_slice.stop}&#34; not in selected files, timerange: &#34;{times_pd.index[0]}&#34; to &#34;{times_pd.index[-1]}&#34;&#39;)
    
    #TODO: check conversion implementation with hydro_tools\ERA5\ERA52DFM.py. Also move to separate function?
    def get_unit(data_xr_var):
        if &#39;units&#39; in data_xr_var.attrs.keys():
            unit = data_xr_var.attrs[&#34;units&#34;]
        else:
            unit = &#39;-&#39;
        return unit
    #convert Kelvin to Celcius
    for varkey_sel in [&#39;air_temperature&#39;,&#39;dew_point_temperature&#39;,&#39;d2m&#39;,&#39;t2m&#39;]: # 2 meter dewpoint temparature / 2 meter temperature
        if varkey_sel in varkeys:
            current_unit = get_unit(data_xr_tsel[varkey_sel])
            new_unit = &#39;C&#39;
            print(f&#39;converting {varkey_sel} unit from Kelvin to Celcius: [{current_unit}] to [{new_unit}]&#39;)
            data_xr_tsel[varkey_sel].attrs[&#39;units&#39;] = new_unit
            data_xr_tsel[varkey_sel] = data_xr_tsel[varkey_sel] - 273.15
    #convert fraction to percentage
    for varkey_sel in [&#39;cloud_area_fraction&#39;,&#39;tcc&#39;]: #total cloud cover
        if varkey_sel in varkeys:
            current_unit = get_unit(data_xr_tsel[varkey_sel])
            new_unit = &#39;%&#39; #unit is soms al %
            print(f&#39;converting {varkey_sel} unit from fraction to percentage: [{current_unit}] to [{new_unit}]&#39;)
            data_xr_tsel[varkey_sel].attrs[&#39;units&#39;] = new_unit
            data_xr_tsel[varkey_sel] = data_xr_tsel[varkey_sel] * 100
    #convert kg/m2/s to mm/day
    for varkey_sel in [&#39;mer&#39;,&#39;mtpr&#39;]: #mean evaporation rate / mean total precipitation rate
        if varkey_sel in varkeys:
            current_unit = get_unit(data_xr_tsel[varkey_sel])
            new_unit = &#39;mm/day&#39;
            print(f&#39;converting {varkey_sel} unit from kg/m2/s to mm/day: [{current_unit}] to [{new_unit}]&#39;)
            data_xr_tsel[varkey_sel].attrs[&#39;units&#39;] = new_unit
            data_xr_tsel[varkey_sel] = data_xr_tsel[varkey_sel] * 86400 # kg/m2/s to mm/day (assuming rho_water=1000)
    #convert J/m2 to W/m2
    for varkey_sel in [&#39;ssr&#39;,&#39;strd&#39;]: #solar influx (surface_net_solar_radiation) / surface_thermal_radiation_downwards
        if varkey_sel in varkeys:
            current_unit = get_unit(data_xr_tsel[varkey_sel])
            new_unit = &#39;W m**-2&#39;
            print(f&#39;converting {varkey_sel} unit from J/m2 to W/m2: [{current_unit}] to [{new_unit}]&#39;)
            data_xr_tsel[varkey_sel].attrs[&#39;units&#39;] = new_unit
            data_xr_tsel[varkey_sel] = data_xr_tsel[varkey_sel] / 3600 # 3600s/h #TODO: 1W = 1J/s, so does not make sense?
    #solar influx increase for beta=6%
    if &#39;ssr&#39; in varkeys:
        print(&#39;ssr (solar influx) increase for beta=6%&#39;)
        data_xr_tsel[&#39;ssr&#39;] = data_xr_tsel[&#39;ssr&#39;] *.94
    
    #convert 0to360 sourcedata to -180to+180
    convert_360to180 = (data_xr[&#39;longitude&#39;].to_numpy()&gt;180).any()
    if convert_360to180:
        data_xr.coords[&#39;longitude&#39;] = (data_xr.coords[&#39;longitude&#39;] + 180) % 360 - 180
        data_xr = data_xr.sortby(data_xr[&#39;longitude&#39;])
    
    #GTSM specific addition for longitude overlap
    if add_global_overlap: # assumes -180 to ~+179.75 (full global extent, but no overlap). Does not seem to mess up results for local models.
        if len(data_xr_tsel.longitude.values) != len(np.unique(data_xr_tsel.longitude.values%360)):
            raise Exception(f&#39;add_global_overlap=True, but there are already overlapping longitude values: {data_xr_tsel.longitude}&#39;)
        overlap_ltor = data_xr_tsel.sel(longitude=data_xr_tsel.longitude&lt;=-179)
        overlap_ltor[&#39;longitude&#39;] = overlap_ltor[&#39;longitude&#39;] + 360
        overlap_rtol = data_xr_tsel.sel(longitude=data_xr_tsel.longitude&gt;=179)
        overlap_rtol[&#39;longitude&#39;] = overlap_rtol[&#39;longitude&#39;] - 360
        data_xr_tsel = xr.concat([data_xr_tsel,overlap_ltor,overlap_rtol],dim=&#39;longitude&#39;).sortby(&#39;longitude&#39;)
    
    #GTSM specific addition for zerovalues during spinup
    if zerostart:
        field_zerostart = data_xr_tsel.isel(time=[0,0])*0 #two times first field, set values to 0
        field_zerostart[&#39;time&#39;] = [times_pd.index[0]-dt.timedelta(days=2),times_pd.index[0]-dt.timedelta(days=1)] #TODO: is one zero field not enough? (is replacing first field not also ok? (results in 1hr transition period)
        data_xr_tsel = xr.concat([field_zerostart,data_xr_tsel],dim=&#39;time&#39;)#.sortby(&#39;time&#39;)
    
    data_xr_tsel = prevent_dtype_int(data_xr_tsel)
    #data_xr_tsel.time.encoding[&#39;units&#39;] = &#39;hours since 1900-01-01 00:00:00&#39; #TODO: maybe add different reftime?
    
    return data_xr_tsel


def Dataset_varswithdim(ds,dimname): #TODO: dit zit ook in xugrid, wordt nu gebruikt in hisfile voorbeeldscript en kan handig zijn, maar misschien die uit xugrid gebruiken?
    if dimname not in ds.dims:
        raise Exception(f&#39;dimension {dimname} not in dataset, available are: {list(ds.dims)}&#39;)
    
    varlist_keep = []
    for varname in ds.variables.keys():
        if dimname in ds[varname].dims:
            varlist_keep.append(varname)
    ds = ds[varlist_keep]
    
    return ds


def get_vertical_dimensions(uds): #TODO: maybe add layer_dimension and interface_dimension properties to xugrid?
    &#34;&#34;&#34;
    get vertical_dimensions from grid_info of ugrid mapfile (this will fail for hisfiles). The info is stored in the layer_dimension and interface_dimension attribute of the mesh2d variable of the dataset (stored in uds.grid after reading with xugrid)
    
    processing cb_3d_map.nc
        &gt;&gt; found layer/interface dimensions in file: mesh2d_nLayers mesh2d_nInterfaces
    processing Grevelingen-FM_0*_map.nc
        &gt;&gt; found layer/interface dimensions in file: nmesh2d_layer nmesh2d_interface (these are updated in open_partitioned_dataset)
    processing DCSM-FM_0_5nm_0*_map.nc
        &gt;&gt; found layer/interface dimensions in file: mesh2d_nLayers mesh2d_nInterfaces
    processing MB_02_0*_map.nc
        &gt;&gt; found layer/interface dimensions in file: mesh2d_nLayers mesh2d_nInterfaces
    &#34;&#34;&#34;
    
    if not hasattr(uds,&#39;grid&#39;): #early return in case of e.g. hisfile
        return None, None
        
    gridname = uds.grid.name
    grid_info = uds.grid.to_dataset()[gridname]
    if hasattr(grid_info,&#39;layer_dimension&#39;):
        print(&#39;&gt;&gt; found layer/interface dimensions in file: &#39;,end=&#39;&#39;)
        print(grid_info.layer_dimension, grid_info.interface_dimension) #combined in attr vertical_dimensions
        return grid_info.layer_dimension, grid_info.interface_dimension
    else:
        return None, None


def remove_ghostcells(uds): #TODO: create JIRA issue: add domainno attribute to partitioned mapfiles or remove ghostcells from output (or make values in ghostcells the same as not-ghostcells)
    &#34;&#34;&#34;
    Dropping ghostcells if there is a domainno variable present and there is a domainno in the filename.
    Not using most-occurring domainno in var, since this is not a valid assumption for merged datasets and might be invalid for a very small partition.
    
    &#34;&#34;&#34;
    gridname = uds.grid.name
    varn_domain = f&#39;{gridname}_flowelem_domain&#39;
    
    #check if dataset has domainno variable, return uds if not present
    if varn_domain not in uds.data_vars:
        print(&#39;[nodomainvar] &#39;,end=&#39;&#39;)
        return uds
    
    #derive domainno from filename, return uds if not present
    fname = uds.encoding[&#39;source&#39;]
    if &#39;_&#39; not in fname: #safety escape in case there is no _ in the filename
        print(&#39;[nodomainfname] &#39;,end=&#39;&#39;)
        return uds
    fname_splitted = fname.split(&#39;_&#39;)
    part_domainno_fromfname = fname_splitted[-2] #this is not valid for rstfiles (date follows after partnumber), but they cannot be read with xugrid anyway since they are mapformat=1
    if not part_domainno_fromfname.isnumeric() or len(part_domainno_fromfname)!=4:
        print(&#39;[nodomainfname] &#39;,end=&#39;&#39;)
        return uds
    
    #drop ghostcells
    part_domainno_fromfname = int(part_domainno_fromfname)
    da_domainno = uds[varn_domain]
    idx = np.flatnonzero(da_domainno == part_domainno_fromfname)
    uds = uds.isel({uds.grid.face_dimension:idx})
    return uds


def open_partitioned_dataset(file_nc, chunks={&#39;time&#39;:1}, remove_ghost=True, **kwargs): 
    &#34;&#34;&#34;
    using xugrid to read and merge partitions, with some additional features (remaning old layerdim, timings, set zcc/zw as data_vars)

    Parameters
    ----------
    file_nc : TYPE
        DESCRIPTION.
    chunks : TYPE, optional
        chunks={&#39;time&#39;:1} increases performance significantly upon reading, but causes memory overloads when performing sum/mean/etc actions over time dimension (in that case 100/200 is better). The default is {&#39;time&#39;:1}.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    ds_merged_xu : TYPE
        DESCRIPTION.
    
    file_nc = &#39;p:\\1204257-dcsmzuno\\2006-2012\\3D-DCSM-FM\\A18b_ntsu1\\DFM_OUTPUT_DCSM-FM_0_5nm\\DCSM-FM_0_5nm_0*_map.nc&#39; #3D DCSM
    file_nc = &#39;p:\\archivedprojects\\11206813-006-kpp2021_rmm-2d\\C_Work\\31_RMM_FMmodel\\computations\\model_setup\\run_207\\results\\RMM_dflowfm_0*_map.nc&#39; #RMM 2D
    file_nc = &#39;p:\\1230882-emodnet_hrsm\\GTSMv5.0\\runs\\reference_GTSMv4.1_wiCA_2.20.06_mapformat4\\output\\gtsm_model_0*_map.nc&#39; #GTSM 2D
    file_nc = &#39;p:\\11208053-005-kpp2022-rmm3d\\C_Work\\01_saltiMarlein\\RMM_2019_computations_02\\computations\\theo_03\\DFM_OUTPUT_RMM_dflowfm_2019\\RMM_dflowfm_2019_0*_map.nc&#39; #RMM 3D
    file_nc = &#39;p:\\archivedprojects\\11203379-005-mwra-updated-bem\\03_model\\02_final\\A72_ntsu0_kzlb2\\DFM_OUTPUT_MB_02\\MB_02_0*_map.nc&#39;
    Timings (xu.open_dataset/xu.merge_partitions):
        - DCSM 3D 20 partitions  367 timesteps: 231.5/ 4.5 sec (decode_times=False: 229.0 sec)
        - RMM  2D  8 partitions  421 timesteps:  55.4/ 4.4 sec (decode_times=False:  56.6 sec)
        - GTSM 2D  8 partitions  746 timesteps:  71.8/30.0 sec (decode_times=False: 204.8 sec)
        - RMM  3D 40 partitions  146 timesteps: 168.8/ 6.3 sec (decode_times=False: 158.4 sec)
        - MWRA 3D 20 partitions 2551 timesteps:  74.4/ 3.4 sec (decode_times=False:  79.0 sec)
    
    &#34;&#34;&#34;
    #TODO: FM-mapfiles contain wgs84/projected_coordinate_system variables. xugrid has .crs property, projected_coordinate_system/wgs84 should be updated to be crs so it will be automatically handled? &gt;&gt; make dflowfm issue (and https://github.com/Deltares/xugrid/issues/42)
    #TODO: add support for multiple grids via keyword? GTSM+riv grid also only contains only one grid, so no testcase available
    #TODO: speed up open_dataset https://github.com/Deltares/dfm_tools/issues/225
    
    dtstart_all = dt.datetime.now()
    file_nc_list = file_to_list(file_nc)
    
    print(f&#39;&gt;&gt; xu.open_dataset() with {len(file_nc_list)} partition(s): &#39;,end=&#39;&#39;)
    dtstart = dt.datetime.now()
    partitions = []
    for iF, file_nc_one in enumerate(file_nc_list):
        print(iF+1,end=&#39; &#39;)
        ds = xr.open_dataset(file_nc_one, chunks=chunks, **kwargs)
        if &#39;nFlowElem&#39; in ds.dims and &#39;nNetElem&#39; in ds.dims: #for mapformat1 mapfiles: merge different face dimensions (rename nFlowElem to nNetElem)
            print(&#39;[mapformat1] &#39;,end=&#39;&#39;)
            ds = ds.rename({&#39;nFlowElem&#39;:&#39;nNetElem&#39;})
        uds = xu.core.wrap.UgridDataset(ds)
        if remove_ghost: #TODO: this makes it way slower (at least for GTSM), but is necessary since values on overlapping cells are not always identical (eg in case of Venice ucmag)
            uds = remove_ghostcells(uds)
        partitions.append(uds)
    print(&#39;: &#39;,end=&#39;&#39;)
    print(f&#39;{(dt.datetime.now()-dtstart).total_seconds():.2f} sec&#39;)
    
    if len(partitions) == 1: #do not merge in case of 1 partition
        return partitions[0]
    
    print(f&#39;&gt;&gt; xu.merge_partitions() with {len(file_nc_list)} partition(s): &#39;,end=&#39;&#39;)
    dtstart = dt.datetime.now()
    ds_merged_xu = xu.merge_partitions(partitions)
    print(f&#39;{(dt.datetime.now()-dtstart).total_seconds():.2f} sec&#39;)
    
    #print variables that are dropped in merging procedure. Often only [&#39;mesh2d_face_x_bnd&#39;, &#39;mesh2d_face_y_bnd&#39;], which can be derived by combining node_coordinates (mesh2d_node_x mesh2d_node_y) and face_node_connectivity (mesh2d_face_nodes). &gt;&gt; can be removed from FM-mapfiles (email of 16-1-2023)
    varlist_onepart = list(partitions[0].variables.keys())
    varlist_merged = list(ds_merged_xu.variables.keys())
    varlist_dropped_bool = ~pd.Series(varlist_onepart).isin(varlist_merged)
    varlist_dropped = pd.Series(varlist_onepart).loc[varlist_dropped_bool]
    if varlist_dropped_bool.any():
        print(f&#39;&gt;&gt; some variables dropped with merging of partitions: {varlist_dropped.tolist()}&#39;)
    
    print(f&#39;&gt;&gt; dfmt.open_partitioned_dataset() total: {(dt.datetime.now()-dtstart_all).total_seconds():.2f} sec&#39;)
    return ds_merged_xu</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dfm_tools.xarray_helpers.file_to_list"><code class="name flex">
<span>def <span class="ident">file_to_list</span></span>(<span>file_nc)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def file_to_list(file_nc):
    if isinstance(file_nc,list):
        file_nc_list = file_nc
    else:
        basename = os.path.basename(file_nc)
        dirname = os.path.dirname(file_nc)
        ext = os.path.splitext(file_nc)[-1]
        if &#39;.*&#39; in basename:
            def glob_re(pattern, strings):
                return list(filter(re.compile(pattern).search, strings))
            file_nc_list = glob_re(basename, glob.glob(os.path.join(dirname,f&#39;*{ext}&#39;)))
        else:
            file_nc_list = glob.glob(file_nc)
        file_nc_list.sort()
    if len(file_nc_list)==0:
        raise Exception(&#39;file(s) not found, empty file_nc_list&#39;)
    return file_nc_list</code></pre>
</details>
</dd>
<dt id="dfm_tools.xarray_helpers.preprocess_hisnc"><code class="name flex">
<span>def <span class="ident">preprocess_hisnc</span></span>(<span>ds)</span>
</code></dt>
<dd>
<div class="desc"><p>Look for dim/coord combination and use this for Dataset.set_index(), to enable station/gs/crs/laterals label based indexing. If duplicate labels are found (like duplicate stations), these are dropped to avoid indexing issues.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ds</code></strong> :&ensp;<code>xarray.Dataset</code></dt>
<dd>DESCRIPTION.</dd>
<dt><strong><code>drop_duplicate_stations</code></strong> :&ensp;<code>TYPE</code>, optional</dt>
<dd>DESCRIPTION. The default is True.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ds</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_hisnc(ds):
    &#34;&#34;&#34;
    Look for dim/coord combination and use this for Dataset.set_index(), to enable station/gs/crs/laterals label based indexing. If duplicate labels are found (like duplicate stations), these are dropped to avoid indexing issues.
    
    Parameters
    ----------
    ds : xarray.Dataset
        DESCRIPTION.
    drop_duplicate_stations : TYPE, optional
        DESCRIPTION. The default is True.

    Returns
    -------
    ds : TYPE
        DESCRIPTION.

    &#34;&#34;&#34;
    
    #generate dim_coord_dict to set indexes, this will be something like {&#39;stations&#39;:&#39;station_name&#39;,&#39;cross_section&#39;:&#39;cross_section_name&#39;} after loop
    dim_coord_dict = {}
    for ds_coord in ds.coords.keys():
        ds_coord_dtype = ds[ds_coord].dtype
        ds_coord_dim = ds[ds_coord].dims[0] #these vars always have only one dim
        if ds_coord_dtype.str.startswith(&#39;|S&#39;): #these are station/crs/laterals/gs names/ids
            dim_coord_dict[ds_coord_dim] = ds_coord
    
    #loop over dimensions and set corresponding coordinates/variables from dim_coord_dict as their index
    for dim in dim_coord_dict.keys():
        coord = dim_coord_dict[dim]
        ds[coord] = ds[coord].load().str.decode(&#39;utf-8&#39;,errors=&#39;ignore&#39;).str.strip() #.load() is essential to convert not only first letter of string.
        ds = ds.set_index({dim:coord})
        
        #drop duplicate indices (stations/crs/gs), this avoids &#34;InvalidIndexError: Reindexing only valid with uniquely valued Index objects&#34;
        duplicated_keepfirst = ds[dim].to_series().duplicated(keep=&#39;first&#39;)
        if duplicated_keepfirst.sum()&gt;0:
            print(f&#39;dropping {duplicated_keepfirst.sum()} duplicate &#34;{coord}&#34; labels to avoid InvalidIndexError&#39;)
            ds = ds[{dim:~duplicated_keepfirst}]

    #check dflowfm version/date and potentially raise warning about incorrect layers
    try:
        source_attr = ds.attrs[&#39;source&#39;] # fails if no source attr present in dataset
        source_attr_version = source_attr.split(&#39;, &#39;)[1]
        source_attr_date = source_attr.split(&#39;, &#39;)[2]
        if pd.Timestamp(source_attr_date) &lt; dt.datetime(2020,11,28):
            warnings.warn(UserWarning(f&#39;Your model was run with a D-FlowFM version from before 28-10-2020 ({source_attr_version} from {source_attr_date}), the layers in the hisfile are incorrect. Check UNST-2920 and UNST-3024 for more information, it was fixed from OSS 67858.&#39;))
    except: #no source attr present in hisfile, cannot check version
        pass

    return ds</code></pre>
</details>
</dd>
<dt id="dfm_tools.xarray_helpers.preprocess_hirlam"><code class="name flex">
<span>def <span class="ident">preprocess_hirlam</span></span>(<span>ds)</span>
</code></dt>
<dd>
<div class="desc"><p>add xy variables as longitude/latitude to avoid duplicate var/dim names (we rename it anyway otherwise)
add xy as variables again with help of NetCDF4
this function is hopefully temporary, necessary since &gt;1D-variables cannot have the same name as one of its dimensions in xarray. Background and future solution: <a href="https://github.com/pydata/xarray/issues/6293">https://github.com/pydata/xarray/issues/6293</a>
"MissingDimensionsError: 'y' has more than 1-dimension and the same name as one of its dimensions ('y', 'x'). xarray disallows such variables because they conflict with the coordinates used to label dimensions."
might be solved in <a href="https://github.com/pydata/xarray/releases/tag/v2023.02.0">https://github.com/pydata/xarray/releases/tag/v2023.02.0</a> (xr.concat), but not certain</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_hirlam(ds):
    &#34;&#34;&#34;
    add xy variables as longitude/latitude to avoid duplicate var/dim names (we rename it anyway otherwise)
    add xy as variables again with help of NetCDF4
    this function is hopefully temporary, necessary since &gt;1D-variables cannot have the same name as one of its dimensions in xarray. Background and future solution: https://github.com/pydata/xarray/issues/6293
    &#34;MissingDimensionsError: &#39;y&#39; has more than 1-dimension and the same name as one of its dimensions (&#39;y&#39;, &#39;x&#39;). xarray disallows such variables because they conflict with the coordinates used to label dimensions.&#34;
    might be solved in https://github.com/pydata/xarray/releases/tag/v2023.02.0 (xr.concat), but not certain
    &#34;&#34;&#34;
    
    print(&#39;hirlam workaround: adding dropped x/y variables again as lon/lat&#39;)
    file_nc_one = ds.encoding[&#39;source&#39;]
    with Dataset(file_nc_one) as data_nc:
        data_nc_x = data_nc[&#39;x&#39;]
        data_nc_y = data_nc[&#39;y&#39;]
        ds[&#39;longitude&#39;] = xr.DataArray(data_nc_x,dims=data_nc_x.dimensions,attrs=data_nc_x.__dict__) 
        ds[&#39;latitude&#39;] = xr.DataArray(data_nc_y,dims=data_nc_y.dimensions,attrs=data_nc_y.__dict__)
    ds = ds.set_coords([&#39;latitude&#39;,&#39;longitude&#39;])
    for varkey in ds.data_vars:
        del ds[varkey].encoding[&#39;coordinates&#39;] #remove {&#39;coordinates&#39;:&#39;y x&#39;} from encoding (otherwise set twice)
    return ds</code></pre>
</details>
</dd>
<dt id="dfm_tools.xarray_helpers.preprocess_ERA5"><code class="name flex">
<span>def <span class="ident">preprocess_ERA5</span></span>(<span>ds)</span>
</code></dt>
<dd>
<div class="desc"><p>Reduces the expver dimension in some of the ERA5 data (mtpr and other variables), which occurs in files with very recent data. The dimension contains the unvalidated data from the latest month in the second index in the expver dimension. The reduction is done with mean, but this is arbitrary, since there is only one valid value per timestep and the other one is nan.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_ERA5(ds):
    &#34;&#34;&#34;
    Reduces the expver dimension in some of the ERA5 data (mtpr and other variables), which occurs in files with very recent data. The dimension contains the unvalidated data from the latest month in the second index in the expver dimension. The reduction is done with mean, but this is arbitrary, since there is only one valid value per timestep and the other one is nan.
    &#34;&#34;&#34;
    if &#39;expver&#39; in ds.dims:
        ds = ds.mean(dim=&#39;expver&#39;)
    return ds</code></pre>
</details>
</dd>
<dt id="dfm_tools.xarray_helpers.preprocess_woa"><code class="name flex">
<span>def <span class="ident">preprocess_woa</span></span>(<span>ds)</span>
</code></dt>
<dd>
<div class="desc"><p>WOA time units is 'months since 0000-01-01 00:00:00' and calendar is not set (360_day is the only calendar that supports that unit in xarray)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_woa(ds):
    &#34;&#34;&#34;
    WOA time units is &#39;months since 0000-01-01 00:00:00&#39; and calendar is not set (360_day is the only calendar that supports that unit in xarray)
    &#34;&#34;&#34;
    ds.time.attrs[&#39;calendar&#39;] = &#39;360_day&#39;
    ds = xr.decode_cf(ds) #decode_cf after adding 360_day calendar attribute
    return ds</code></pre>
</details>
</dd>
<dt id="dfm_tools.xarray_helpers.prevent_dtype_int"><code class="name flex">
<span>def <span class="ident">prevent_dtype_int</span></span>(<span>ds)</span>
</code></dt>
<dd>
<div class="desc"><p>Prevent writing to int, since it might mess up dataset (<a href="https://github.com/Deltares/dfm_tools/issues/239">https://github.com/Deltares/dfm_tools/issues/239</a>)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prevent_dtype_int(ds):
    &#34;&#34;&#34;
    Prevent writing to int, since it might mess up dataset (https://github.com/Deltares/dfm_tools/issues/239)
    &#34;&#34;&#34;
    for var in ds.data_vars:
        var_encoding = ds[var].encoding
        if &#39;dtype&#39; in var_encoding.keys():
            if &#39;int&#39; in str(var_encoding[&#39;dtype&#39;]):
                ds[var].encoding[&#39;dtype&#39;] = np.dtype(&#39;float32&#39;)
    return ds</code></pre>
</details>
</dd>
<dt id="dfm_tools.xarray_helpers.merge_meteofiles"><code class="name flex">
<span>def <span class="ident">merge_meteofiles</span></span>(<span>file_nc:Â str, preprocess=None, chunks:Â dictÂ =Â {'time': 1}, time_slice:Â sliceÂ =Â slice(None, None, None), add_global_overlap:Â boolÂ =Â False, zerostart:Â boolÂ =Â False) â€‘>Â xarray.core.dataset.Dataset</span>
</code></dt>
<dd>
<div class="desc"><p>for merging for instance meteo files
x/y and lon/lat are renamed to longitude/latitude #TODO: is this desireable?</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_nc</code></strong> :&ensp;<code>str</code></dt>
<dd>DESCRIPTION.</dd>
<dt><strong><code>preprocess</code></strong> :&ensp;<code>TYPE</code>, optional</dt>
<dd>DESCRIPTION. The default is None.</dd>
<dt><strong><code>chunks</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Prevents large chunks and memory issues. The default is {'time':1}.</dd>
<dt><strong><code>time_slice</code></strong> :&ensp;<code>slice</code>, optional</dt>
<dd>DESCRIPTION. The default is slice(None,None).</dd>
<dt><strong><code>add_global_overlap</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>GTSM specific: extend data beyond -180 to 180 longitude. The default is False.</dd>
<dt><strong><code>zerostart</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>GTSM specific: extend data with 0-value fields 1 and 2 days before time_slice.start. The default is False.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_meteofiles(file_nc:str, preprocess=None, 
                     chunks:dict = {&#39;time&#39;:1},
                     time_slice:slice = slice(None,None),
                     add_global_overlap:bool = False, zerostart:bool = False) -&gt; xr.Dataset:
    &#34;&#34;&#34;
    for merging for instance meteo files
    x/y and lon/lat are renamed to longitude/latitude #TODO: is this desireable?

    Parameters
    ----------
    file_nc : str
        DESCRIPTION.
    preprocess : TYPE, optional
        DESCRIPTION. The default is None.
    chunks : dict, optional
        Prevents large chunks and memory issues. The default is {&#39;time&#39;:1}.
    time_slice : slice, optional
        DESCRIPTION. The default is slice(None,None).
    add_global_overlap : bool, optional
        GTSM specific: extend data beyond -180 to 180 longitude. The default is False.
    zerostart : bool, optional
        GTSM specific: extend data with 0-value fields 1 and 2 days before time_slice.start. The default is False.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    TYPE
        DESCRIPTION.

    &#34;&#34;&#34;
    #TODO: add ERA5 conversions and features from hydro_tools\ERA5\ERA52DFM.py (except for varRhoair_alt, request FM support for varying airpressure: https://issuetracker.deltares.nl/browse/UNST-6593)
    #TODO: request FM support for charnock (etc) separate meteo forcing (currently airpressure_windx_windy_charnock merged file is required): https://issuetracker.deltares.nl/browse/UNST-6453
    #TODO: provide extfile example with fmquantity/ncvarname combinations and cleanup FM code: https://issuetracker.deltares.nl/browse/UNST-6453
    #TODO: add coordinate conversion (only valid for models with multidimensional lat/lon variables like HARMONIE and HIRLAM). This should work: ds_reproj = ds.set_crs(4326).to_crs(28992)
    #TODO: add CMCC etc from gtsmip repos (mainly calendar conversion)
    #TODO: put conversions in separate function?
    #TODO: maybe add renaming like {&#39;salinity&#39;:&#39;so&#39;, &#39;water_temp&#39;:&#39;thetao&#39;} for hycom
    
    #hirlam workaround
    if preprocess == preprocess_hirlam:
        drop_variables = [&#39;x&#39;,&#39;y&#39;] #will be added again as longitude/latitude, this is a workaround (see dfmt.preprocess_hirlam for details)
    else:
        drop_variables = None
    #woa workaround
    if preprocess == preprocess_woa:
        decode_cf = False
    else:
        decode_cf = True        

    file_nc_list = file_to_list(file_nc)

    print(f&#39;&gt;&gt; opening multifile dataset of {len(file_nc_list)} files (can take a while with lots of files): &#39;,end=&#39;&#39;)
    dtstart = dt.datetime.now()
    data_xr = xr.open_mfdataset(file_nc_list,
                                parallel=True, #speeds up the process
                                preprocess=preprocess,
                                chunks=chunks,
                                drop_variables=drop_variables, #necessary since dims/vars with equal names are not allowed by xarray, add again later and requested matroos to adjust netcdf format.
                                decode_cf=decode_cf)
    print(f&#39;{(dt.datetime.now()-dtstart).total_seconds():.2f} sec&#39;)
    
    #rename variables
    if not &#39;longitude&#39; in data_xr.variables: #TODO: make generic, comparable rename in rename_dims_dict in dfmt.open_dataset_extra()
        if &#39;lon&#39; in data_xr.variables:
            data_xr = data_xr.rename({&#39;lon&#39;:&#39;longitude&#39;, &#39;lat&#39;:&#39;latitude&#39;})
        elif &#39;x&#39; in data_xr.variables:
            data_xr = data_xr.rename({&#39;x&#39;:&#39;longitude&#39;, &#39;y&#39;:&#39;latitude&#39;})
        else:
            raise Exception(&#39;no longitude/latitude, lon/lat or x/y variables found in dataset&#39;)

    varkeys = data_xr.variables.mapping.keys()
    #data_xr.attrs[&#39;comment&#39;] = &#39;merged with dfm_tools from https://github.com/Deltares/dfm_tools&#39; #TODO: add something like this or other attributes? (some might also be dropped now)
    
    #select time and do checks #TODO: check if calendar is standard/gregorian
    print(&#39;time selection&#39;)
    data_xr_tsel = data_xr.sel(time=time_slice)
    if data_xr_tsel.get_index(&#39;time&#39;).duplicated().any():
        print(&#39;dropping duplicate timesteps&#39;)
        data_xr_tsel = data_xr_tsel.sel(time=~data_xr_tsel.get_index(&#39;time&#39;).duplicated()) #drop duplicate timesteps
    
    #check if there are times selected
    if len(data_xr_tsel.time)==0:
        raise Exception(f&#39;ERROR: no times selected, ds_text={data_xr.time[[0,-1]].to_numpy()} and time_slice={time_slice}&#39;)
    
    #check if there are no gaps (more than one unique timestep)
    times_pd = data_xr_tsel[&#39;time&#39;].to_series()
    timesteps_uniq = times_pd.diff().iloc[1:].unique()
    if len(timesteps_uniq)&gt;1:
        raise Exception(f&#39;ERROR: gaps found in selected dataset (are there sourcefiles missing?), unique timesteps (hour): {timesteps_uniq/1e9/3600}&#39;)
    
    #check if requested times are available in selected files (in times_pd)
    if not time_slice.start in times_pd.index:
        raise Exception(f&#39;ERROR: time_slice_start=&#34;{time_slice.start}&#34; not in selected files, timerange: &#34;{times_pd.index[0]}&#34; to &#34;{times_pd.index[-1]}&#34;&#39;)
    if not time_slice.stop in times_pd.index:
        raise Exception(f&#39;ERROR: time_slice_stop=&#34;{time_slice.stop}&#34; not in selected files, timerange: &#34;{times_pd.index[0]}&#34; to &#34;{times_pd.index[-1]}&#34;&#39;)
    
    #TODO: check conversion implementation with hydro_tools\ERA5\ERA52DFM.py. Also move to separate function?
    def get_unit(data_xr_var):
        if &#39;units&#39; in data_xr_var.attrs.keys():
            unit = data_xr_var.attrs[&#34;units&#34;]
        else:
            unit = &#39;-&#39;
        return unit
    #convert Kelvin to Celcius
    for varkey_sel in [&#39;air_temperature&#39;,&#39;dew_point_temperature&#39;,&#39;d2m&#39;,&#39;t2m&#39;]: # 2 meter dewpoint temparature / 2 meter temperature
        if varkey_sel in varkeys:
            current_unit = get_unit(data_xr_tsel[varkey_sel])
            new_unit = &#39;C&#39;
            print(f&#39;converting {varkey_sel} unit from Kelvin to Celcius: [{current_unit}] to [{new_unit}]&#39;)
            data_xr_tsel[varkey_sel].attrs[&#39;units&#39;] = new_unit
            data_xr_tsel[varkey_sel] = data_xr_tsel[varkey_sel] - 273.15
    #convert fraction to percentage
    for varkey_sel in [&#39;cloud_area_fraction&#39;,&#39;tcc&#39;]: #total cloud cover
        if varkey_sel in varkeys:
            current_unit = get_unit(data_xr_tsel[varkey_sel])
            new_unit = &#39;%&#39; #unit is soms al %
            print(f&#39;converting {varkey_sel} unit from fraction to percentage: [{current_unit}] to [{new_unit}]&#39;)
            data_xr_tsel[varkey_sel].attrs[&#39;units&#39;] = new_unit
            data_xr_tsel[varkey_sel] = data_xr_tsel[varkey_sel] * 100
    #convert kg/m2/s to mm/day
    for varkey_sel in [&#39;mer&#39;,&#39;mtpr&#39;]: #mean evaporation rate / mean total precipitation rate
        if varkey_sel in varkeys:
            current_unit = get_unit(data_xr_tsel[varkey_sel])
            new_unit = &#39;mm/day&#39;
            print(f&#39;converting {varkey_sel} unit from kg/m2/s to mm/day: [{current_unit}] to [{new_unit}]&#39;)
            data_xr_tsel[varkey_sel].attrs[&#39;units&#39;] = new_unit
            data_xr_tsel[varkey_sel] = data_xr_tsel[varkey_sel] * 86400 # kg/m2/s to mm/day (assuming rho_water=1000)
    #convert J/m2 to W/m2
    for varkey_sel in [&#39;ssr&#39;,&#39;strd&#39;]: #solar influx (surface_net_solar_radiation) / surface_thermal_radiation_downwards
        if varkey_sel in varkeys:
            current_unit = get_unit(data_xr_tsel[varkey_sel])
            new_unit = &#39;W m**-2&#39;
            print(f&#39;converting {varkey_sel} unit from J/m2 to W/m2: [{current_unit}] to [{new_unit}]&#39;)
            data_xr_tsel[varkey_sel].attrs[&#39;units&#39;] = new_unit
            data_xr_tsel[varkey_sel] = data_xr_tsel[varkey_sel] / 3600 # 3600s/h #TODO: 1W = 1J/s, so does not make sense?
    #solar influx increase for beta=6%
    if &#39;ssr&#39; in varkeys:
        print(&#39;ssr (solar influx) increase for beta=6%&#39;)
        data_xr_tsel[&#39;ssr&#39;] = data_xr_tsel[&#39;ssr&#39;] *.94
    
    #convert 0to360 sourcedata to -180to+180
    convert_360to180 = (data_xr[&#39;longitude&#39;].to_numpy()&gt;180).any()
    if convert_360to180:
        data_xr.coords[&#39;longitude&#39;] = (data_xr.coords[&#39;longitude&#39;] + 180) % 360 - 180
        data_xr = data_xr.sortby(data_xr[&#39;longitude&#39;])
    
    #GTSM specific addition for longitude overlap
    if add_global_overlap: # assumes -180 to ~+179.75 (full global extent, but no overlap). Does not seem to mess up results for local models.
        if len(data_xr_tsel.longitude.values) != len(np.unique(data_xr_tsel.longitude.values%360)):
            raise Exception(f&#39;add_global_overlap=True, but there are already overlapping longitude values: {data_xr_tsel.longitude}&#39;)
        overlap_ltor = data_xr_tsel.sel(longitude=data_xr_tsel.longitude&lt;=-179)
        overlap_ltor[&#39;longitude&#39;] = overlap_ltor[&#39;longitude&#39;] + 360
        overlap_rtol = data_xr_tsel.sel(longitude=data_xr_tsel.longitude&gt;=179)
        overlap_rtol[&#39;longitude&#39;] = overlap_rtol[&#39;longitude&#39;] - 360
        data_xr_tsel = xr.concat([data_xr_tsel,overlap_ltor,overlap_rtol],dim=&#39;longitude&#39;).sortby(&#39;longitude&#39;)
    
    #GTSM specific addition for zerovalues during spinup
    if zerostart:
        field_zerostart = data_xr_tsel.isel(time=[0,0])*0 #two times first field, set values to 0
        field_zerostart[&#39;time&#39;] = [times_pd.index[0]-dt.timedelta(days=2),times_pd.index[0]-dt.timedelta(days=1)] #TODO: is one zero field not enough? (is replacing first field not also ok? (results in 1hr transition period)
        data_xr_tsel = xr.concat([field_zerostart,data_xr_tsel],dim=&#39;time&#39;)#.sortby(&#39;time&#39;)
    
    data_xr_tsel = prevent_dtype_int(data_xr_tsel)
    #data_xr_tsel.time.encoding[&#39;units&#39;] = &#39;hours since 1900-01-01 00:00:00&#39; #TODO: maybe add different reftime?
    
    return data_xr_tsel</code></pre>
</details>
</dd>
<dt id="dfm_tools.xarray_helpers.Dataset_varswithdim"><code class="name flex">
<span>def <span class="ident">Dataset_varswithdim</span></span>(<span>ds, dimname)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Dataset_varswithdim(ds,dimname): #TODO: dit zit ook in xugrid, wordt nu gebruikt in hisfile voorbeeldscript en kan handig zijn, maar misschien die uit xugrid gebruiken?
    if dimname not in ds.dims:
        raise Exception(f&#39;dimension {dimname} not in dataset, available are: {list(ds.dims)}&#39;)
    
    varlist_keep = []
    for varname in ds.variables.keys():
        if dimname in ds[varname].dims:
            varlist_keep.append(varname)
    ds = ds[varlist_keep]
    
    return ds</code></pre>
</details>
</dd>
<dt id="dfm_tools.xarray_helpers.get_vertical_dimensions"><code class="name flex">
<span>def <span class="ident">get_vertical_dimensions</span></span>(<span>uds)</span>
</code></dt>
<dd>
<div class="desc"><p>get vertical_dimensions from grid_info of ugrid mapfile (this will fail for hisfiles). The info is stored in the layer_dimension and interface_dimension attribute of the mesh2d variable of the dataset (stored in uds.grid after reading with xugrid)</p>
<p>processing cb_3d_map.nc
&gt;&gt; found layer/interface dimensions in file: mesh2d_nLayers mesh2d_nInterfaces
processing Grevelingen-FM_0<em>_map.nc
&gt;&gt; found layer/interface dimensions in file: nmesh2d_layer nmesh2d_interface (these are updated in open_partitioned_dataset)
processing DCSM-FM_0_5nm_0</em>_map.nc
&gt;&gt; found layer/interface dimensions in file: mesh2d_nLayers mesh2d_nInterfaces
processing MB_02_0*_map.nc
&gt;&gt; found layer/interface dimensions in file: mesh2d_nLayers mesh2d_nInterfaces</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_vertical_dimensions(uds): #TODO: maybe add layer_dimension and interface_dimension properties to xugrid?
    &#34;&#34;&#34;
    get vertical_dimensions from grid_info of ugrid mapfile (this will fail for hisfiles). The info is stored in the layer_dimension and interface_dimension attribute of the mesh2d variable of the dataset (stored in uds.grid after reading with xugrid)
    
    processing cb_3d_map.nc
        &gt;&gt; found layer/interface dimensions in file: mesh2d_nLayers mesh2d_nInterfaces
    processing Grevelingen-FM_0*_map.nc
        &gt;&gt; found layer/interface dimensions in file: nmesh2d_layer nmesh2d_interface (these are updated in open_partitioned_dataset)
    processing DCSM-FM_0_5nm_0*_map.nc
        &gt;&gt; found layer/interface dimensions in file: mesh2d_nLayers mesh2d_nInterfaces
    processing MB_02_0*_map.nc
        &gt;&gt; found layer/interface dimensions in file: mesh2d_nLayers mesh2d_nInterfaces
    &#34;&#34;&#34;
    
    if not hasattr(uds,&#39;grid&#39;): #early return in case of e.g. hisfile
        return None, None
        
    gridname = uds.grid.name
    grid_info = uds.grid.to_dataset()[gridname]
    if hasattr(grid_info,&#39;layer_dimension&#39;):
        print(&#39;&gt;&gt; found layer/interface dimensions in file: &#39;,end=&#39;&#39;)
        print(grid_info.layer_dimension, grid_info.interface_dimension) #combined in attr vertical_dimensions
        return grid_info.layer_dimension, grid_info.interface_dimension
    else:
        return None, None</code></pre>
</details>
</dd>
<dt id="dfm_tools.xarray_helpers.remove_ghostcells"><code class="name flex">
<span>def <span class="ident">remove_ghostcells</span></span>(<span>uds)</span>
</code></dt>
<dd>
<div class="desc"><p>Dropping ghostcells if there is a domainno variable present and there is a domainno in the filename.
Not using most-occurring domainno in var, since this is not a valid assumption for merged datasets and might be invalid for a very small partition.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_ghostcells(uds): #TODO: create JIRA issue: add domainno attribute to partitioned mapfiles or remove ghostcells from output (or make values in ghostcells the same as not-ghostcells)
    &#34;&#34;&#34;
    Dropping ghostcells if there is a domainno variable present and there is a domainno in the filename.
    Not using most-occurring domainno in var, since this is not a valid assumption for merged datasets and might be invalid for a very small partition.
    
    &#34;&#34;&#34;
    gridname = uds.grid.name
    varn_domain = f&#39;{gridname}_flowelem_domain&#39;
    
    #check if dataset has domainno variable, return uds if not present
    if varn_domain not in uds.data_vars:
        print(&#39;[nodomainvar] &#39;,end=&#39;&#39;)
        return uds
    
    #derive domainno from filename, return uds if not present
    fname = uds.encoding[&#39;source&#39;]
    if &#39;_&#39; not in fname: #safety escape in case there is no _ in the filename
        print(&#39;[nodomainfname] &#39;,end=&#39;&#39;)
        return uds
    fname_splitted = fname.split(&#39;_&#39;)
    part_domainno_fromfname = fname_splitted[-2] #this is not valid for rstfiles (date follows after partnumber), but they cannot be read with xugrid anyway since they are mapformat=1
    if not part_domainno_fromfname.isnumeric() or len(part_domainno_fromfname)!=4:
        print(&#39;[nodomainfname] &#39;,end=&#39;&#39;)
        return uds
    
    #drop ghostcells
    part_domainno_fromfname = int(part_domainno_fromfname)
    da_domainno = uds[varn_domain]
    idx = np.flatnonzero(da_domainno == part_domainno_fromfname)
    uds = uds.isel({uds.grid.face_dimension:idx})
    return uds</code></pre>
</details>
</dd>
<dt id="dfm_tools.xarray_helpers.open_partitioned_dataset"><code class="name flex">
<span>def <span class="ident">open_partitioned_dataset</span></span>(<span>file_nc, chunks={'time': 1}, remove_ghost=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>using xugrid to read and merge partitions, with some additional features (remaning old layerdim, timings, set zcc/zw as data_vars)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_nc</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
<dt><strong><code>chunks</code></strong> :&ensp;<code>TYPE</code>, optional</dt>
<dd>chunks={'time':1} increases performance significantly upon reading, but causes memory overloads when performing sum/mean/etc actions over time dimension (in that case 100/200 is better). The default is {'time':1}.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>DESCRIPTION.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ds_merged_xu</code></strong> :&ensp;<code>TYPE</code></dt>
<dd>DESCRIPTION.</dd>
<dt>file_nc = 'p:\1204257-dcsmzuno\2006-2012\3D-DCSM-FM\A18b_ntsu1\DFM_OUTPUT_DCSM-FM_0_5nm\DCSM-FM_0_5nm_0*_map.nc' #3D DCSM</dt>
<dt>file_nc = 'p:\archivedprojects\11206813-006-kpp2021_rmm-2d\C_Work\31_RMM_FMmodel\computations\model_setup\run_207\results\RMM_dflowfm_0*_map.nc' #RMM 2D</dt>
<dt>file_nc = 'p:\1230882-emodnet_hrsm\GTSMv5.0\runs\reference_GTSMv4.1_wiCA_2.20.06_mapformat4\output\gtsm_model_0*_map.nc' #GTSM 2D</dt>
<dt>file_nc = 'p:\11208053-005-kpp2022-rmm3d\C_Work\01_saltiMarlein\RMM_2019_computations_02\computations\theo_03\DFM_OUTPUT_RMM_dflowfm_2019\RMM_dflowfm_2019_0*_map.nc' #RMM 3D</dt>
<dt>file_nc = 'p:\archivedprojects\11203379-005-mwra-updated-bem\03_model\02_final\A72_ntsu0_kzlb2\DFM_OUTPUT_MB_02\MB_02_0*_map.nc'</dt>
<dt><code>Timings (xu.open_dataset/xu.merge_partitions):</code></dt>
<dd>
<ul>
<li>DCSM 3D 20 partitions
367 timesteps: 231.5/ 4.5 sec (decode_times=False: 229.0 sec)</li>
<li>RMM
2D
8 partitions
421 timesteps:
55.4/ 4.4 sec (decode_times=False:
56.6 sec)</li>
<li>GTSM 2D
8 partitions
746 timesteps:
71.8/30.0 sec (decode_times=False: 204.8 sec)</li>
<li>RMM
3D 40 partitions
146 timesteps: 168.8/ 6.3 sec (decode_times=False: 158.4 sec)</li>
<li>MWRA 3D 20 partitions 2551 timesteps:
74.4/ 3.4 sec (decode_times=False:
79.0 sec)</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def open_partitioned_dataset(file_nc, chunks={&#39;time&#39;:1}, remove_ghost=True, **kwargs): 
    &#34;&#34;&#34;
    using xugrid to read and merge partitions, with some additional features (remaning old layerdim, timings, set zcc/zw as data_vars)

    Parameters
    ----------
    file_nc : TYPE
        DESCRIPTION.
    chunks : TYPE, optional
        chunks={&#39;time&#39;:1} increases performance significantly upon reading, but causes memory overloads when performing sum/mean/etc actions over time dimension (in that case 100/200 is better). The default is {&#39;time&#39;:1}.

    Raises
    ------
    Exception
        DESCRIPTION.

    Returns
    -------
    ds_merged_xu : TYPE
        DESCRIPTION.
    
    file_nc = &#39;p:\\1204257-dcsmzuno\\2006-2012\\3D-DCSM-FM\\A18b_ntsu1\\DFM_OUTPUT_DCSM-FM_0_5nm\\DCSM-FM_0_5nm_0*_map.nc&#39; #3D DCSM
    file_nc = &#39;p:\\archivedprojects\\11206813-006-kpp2021_rmm-2d\\C_Work\\31_RMM_FMmodel\\computations\\model_setup\\run_207\\results\\RMM_dflowfm_0*_map.nc&#39; #RMM 2D
    file_nc = &#39;p:\\1230882-emodnet_hrsm\\GTSMv5.0\\runs\\reference_GTSMv4.1_wiCA_2.20.06_mapformat4\\output\\gtsm_model_0*_map.nc&#39; #GTSM 2D
    file_nc = &#39;p:\\11208053-005-kpp2022-rmm3d\\C_Work\\01_saltiMarlein\\RMM_2019_computations_02\\computations\\theo_03\\DFM_OUTPUT_RMM_dflowfm_2019\\RMM_dflowfm_2019_0*_map.nc&#39; #RMM 3D
    file_nc = &#39;p:\\archivedprojects\\11203379-005-mwra-updated-bem\\03_model\\02_final\\A72_ntsu0_kzlb2\\DFM_OUTPUT_MB_02\\MB_02_0*_map.nc&#39;
    Timings (xu.open_dataset/xu.merge_partitions):
        - DCSM 3D 20 partitions  367 timesteps: 231.5/ 4.5 sec (decode_times=False: 229.0 sec)
        - RMM  2D  8 partitions  421 timesteps:  55.4/ 4.4 sec (decode_times=False:  56.6 sec)
        - GTSM 2D  8 partitions  746 timesteps:  71.8/30.0 sec (decode_times=False: 204.8 sec)
        - RMM  3D 40 partitions  146 timesteps: 168.8/ 6.3 sec (decode_times=False: 158.4 sec)
        - MWRA 3D 20 partitions 2551 timesteps:  74.4/ 3.4 sec (decode_times=False:  79.0 sec)
    
    &#34;&#34;&#34;
    #TODO: FM-mapfiles contain wgs84/projected_coordinate_system variables. xugrid has .crs property, projected_coordinate_system/wgs84 should be updated to be crs so it will be automatically handled? &gt;&gt; make dflowfm issue (and https://github.com/Deltares/xugrid/issues/42)
    #TODO: add support for multiple grids via keyword? GTSM+riv grid also only contains only one grid, so no testcase available
    #TODO: speed up open_dataset https://github.com/Deltares/dfm_tools/issues/225
    
    dtstart_all = dt.datetime.now()
    file_nc_list = file_to_list(file_nc)
    
    print(f&#39;&gt;&gt; xu.open_dataset() with {len(file_nc_list)} partition(s): &#39;,end=&#39;&#39;)
    dtstart = dt.datetime.now()
    partitions = []
    for iF, file_nc_one in enumerate(file_nc_list):
        print(iF+1,end=&#39; &#39;)
        ds = xr.open_dataset(file_nc_one, chunks=chunks, **kwargs)
        if &#39;nFlowElem&#39; in ds.dims and &#39;nNetElem&#39; in ds.dims: #for mapformat1 mapfiles: merge different face dimensions (rename nFlowElem to nNetElem)
            print(&#39;[mapformat1] &#39;,end=&#39;&#39;)
            ds = ds.rename({&#39;nFlowElem&#39;:&#39;nNetElem&#39;})
        uds = xu.core.wrap.UgridDataset(ds)
        if remove_ghost: #TODO: this makes it way slower (at least for GTSM), but is necessary since values on overlapping cells are not always identical (eg in case of Venice ucmag)
            uds = remove_ghostcells(uds)
        partitions.append(uds)
    print(&#39;: &#39;,end=&#39;&#39;)
    print(f&#39;{(dt.datetime.now()-dtstart).total_seconds():.2f} sec&#39;)
    
    if len(partitions) == 1: #do not merge in case of 1 partition
        return partitions[0]
    
    print(f&#39;&gt;&gt; xu.merge_partitions() with {len(file_nc_list)} partition(s): &#39;,end=&#39;&#39;)
    dtstart = dt.datetime.now()
    ds_merged_xu = xu.merge_partitions(partitions)
    print(f&#39;{(dt.datetime.now()-dtstart).total_seconds():.2f} sec&#39;)
    
    #print variables that are dropped in merging procedure. Often only [&#39;mesh2d_face_x_bnd&#39;, &#39;mesh2d_face_y_bnd&#39;], which can be derived by combining node_coordinates (mesh2d_node_x mesh2d_node_y) and face_node_connectivity (mesh2d_face_nodes). &gt;&gt; can be removed from FM-mapfiles (email of 16-1-2023)
    varlist_onepart = list(partitions[0].variables.keys())
    varlist_merged = list(ds_merged_xu.variables.keys())
    varlist_dropped_bool = ~pd.Series(varlist_onepart).isin(varlist_merged)
    varlist_dropped = pd.Series(varlist_onepart).loc[varlist_dropped_bool]
    if varlist_dropped_bool.any():
        print(f&#39;&gt;&gt; some variables dropped with merging of partitions: {varlist_dropped.tolist()}&#39;)
    
    print(f&#39;&gt;&gt; dfmt.open_partitioned_dataset() total: {(dt.datetime.now()-dtstart_all).total_seconds():.2f} sec&#39;)
    return ds_merged_xu</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dfm_tools" href="index.html">dfm_tools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="dfm_tools.xarray_helpers.file_to_list" href="#dfm_tools.xarray_helpers.file_to_list">file_to_list</a></code></li>
<li><code><a title="dfm_tools.xarray_helpers.preprocess_hisnc" href="#dfm_tools.xarray_helpers.preprocess_hisnc">preprocess_hisnc</a></code></li>
<li><code><a title="dfm_tools.xarray_helpers.preprocess_hirlam" href="#dfm_tools.xarray_helpers.preprocess_hirlam">preprocess_hirlam</a></code></li>
<li><code><a title="dfm_tools.xarray_helpers.preprocess_ERA5" href="#dfm_tools.xarray_helpers.preprocess_ERA5">preprocess_ERA5</a></code></li>
<li><code><a title="dfm_tools.xarray_helpers.preprocess_woa" href="#dfm_tools.xarray_helpers.preprocess_woa">preprocess_woa</a></code></li>
<li><code><a title="dfm_tools.xarray_helpers.prevent_dtype_int" href="#dfm_tools.xarray_helpers.prevent_dtype_int">prevent_dtype_int</a></code></li>
<li><code><a title="dfm_tools.xarray_helpers.merge_meteofiles" href="#dfm_tools.xarray_helpers.merge_meteofiles">merge_meteofiles</a></code></li>
<li><code><a title="dfm_tools.xarray_helpers.Dataset_varswithdim" href="#dfm_tools.xarray_helpers.Dataset_varswithdim">Dataset_varswithdim</a></code></li>
<li><code><a title="dfm_tools.xarray_helpers.get_vertical_dimensions" href="#dfm_tools.xarray_helpers.get_vertical_dimensions">get_vertical_dimensions</a></code></li>
<li><code><a title="dfm_tools.xarray_helpers.remove_ghostcells" href="#dfm_tools.xarray_helpers.remove_ghostcells">remove_ghostcells</a></code></li>
<li><code><a title="dfm_tools.xarray_helpers.open_partitioned_dataset" href="#dfm_tools.xarray_helpers.open_partitioned_dataset">open_partitioned_dataset</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>